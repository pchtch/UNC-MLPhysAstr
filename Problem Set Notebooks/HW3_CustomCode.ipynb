{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44a4b043",
   "metadata": {},
   "source": [
    "### Coding your own methods.\n",
    "\n",
    "Sometimes, sk-learn (or other libraries) may not have what you need for your problem, or may need some additional input to give what you need.\n",
    "In research contexts, you may often want to write your own custom functions for linear regression. \n",
    "This notebook walks you through coding some methods \"by hand\" (with and without assistance from sk-learn). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b3124e",
   "metadata": {},
   "source": [
    "We'll be working with the same data we used in class, with 1 feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8346dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model #New!\n",
    "\n",
    "font = {'size'   : 16}\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14) \n",
    "#matplotlib.rcParams.update({'figure.autolayout': False})\n",
    "matplotlib.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb28196",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(16) #set seed for reproducibility purposes\n",
    "\n",
    "x = np.arange(100) \n",
    "\n",
    "yp = 3*x + 3 + 2*(np.random.poisson(3*x+3,100)-(3*x+3)) #generate some data with scatter following Poisson distribution \n",
    "                                                    #with exp value = y from linear model, centered around 0\n",
    "\n",
    "np.random.seed(12) #set \n",
    "out = np.random.choice(100,15) #select 15 outliers indexes\n",
    "yp_wo = np.copy(yp)\n",
    "np.random.seed(12) #set again\n",
    "yp_wo[out] = yp_wo[out] + 5*np.random.rand(15)*yp[out]\n",
    "\n",
    "plt.scatter(x, yp_wo);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00e8df8",
   "metadata": {},
   "source": [
    "### Custom scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3967d6c",
   "metadata": {},
   "source": [
    "In studio, we saw that the LinearRegression model allowed us to select from a list of scoring options. We used MSE and MAE.\n",
    "\n",
    "We might like to implement a scorer where we care about percentage error instead. Here is how to do a custom scorer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47761007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df98223",
   "metadata": {},
   "source": [
    "Add your own function definition below to return the mean absolute percent error:\n",
    "\n",
    "$$ MAPE = \\frac{100}{n} \\Sigma_{i=1}^n |\\frac{y_i - \\hat{y}_i}{y_i}| $$\n",
    "\n",
    "where $y_i$'s are the true values and $\\hat{y}_i$'s are the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77ee4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(true,pred): #Modified Mean Absolute Percentage Error\n",
    "    return # your code here\n",
    "\n",
    "mape_scorer = make_scorer(mape, greater_is_better = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9c4d1d",
   "metadata": {},
   "source": [
    "Train a linear regression model using cross-validation. The new $\\texttt{mape\\_scorer}$ you made can be passed to the \"scoring\" parameter as usual. \n",
    "\n",
    "Give the average test error with uncertainty for the built-in RMSE scorer and your hand-made MAPE scorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25d48cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code to train a LinearRegression model and give RMSE and MAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c750aa6e",
   "metadata": {},
   "source": [
    "### Gradient Descent Methods\n",
    "\n",
    "While sk-learn has a built-in stochastic gradient descent module, it doesn't have batch or mini-batch gradient descent (though you can use SGD in a way that approximates those methods). To see how these methods work, we'll code them by hand. That will also let us visualize what they're doing, which we can't do when the steps happen \"under the hood\" in sk-learn.\n",
    "\n",
    "We'll do this for MSE, since that allows us to calculate the gradients analytically. For other loss functions, gradients may need to be calculated numerically. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424ad38e",
   "metadata": {},
   "source": [
    "A reminder from class:\n",
    "\n",
    "$$ MSE = \\frac{1}{N} |X \\cdot \\beta -y |^2$$\n",
    "\n",
    "$$ \\nabla_\\beta MSE = \\frac{2}{N} X^T(X \\cdot \\beta -y )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c001c1",
   "metadata": {},
   "source": [
    "Ultimately, we want to compare the results we get from Gradient Descent methods to the results found using the normal equation. The LinearRegression model you trained above uses the normal equation to solve, so save the coefficients from it to an array for later comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa6f2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code to get and save the coefficients for comparison. What happens if you try to do this with cross-validation?\n",
    "\n",
    "beta_ne = np.array([[intercept],[slope[0]]]) # once you have the coefficients, save them in this form for later computation steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a5af9c",
   "metadata": {},
   "source": [
    "### Let's now implement the simplest form of gradient descent: batch, stochastic, and mini-batch, one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e426d4c0",
   "metadata": {},
   "source": [
    "To begin with, we add x0 = 1 to each instance; this is the bias term and it is used in order to write the solution for a linear model in matrix multiplication form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa00f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones((100, 1)), x]  \n",
    "\n",
    "print(X.shape) #shape is number of instances x number of parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74492c9",
   "metadata": {},
   "source": [
    "We can manually calculate the loss from the result of the normal equation. I'll do this one for you, so it can serve as an example for later steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bffb287",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ne = np.mean((X.dot(beta_ne) - yp_wo.reshape(-1,1))**2)\n",
    "\n",
    "loss_ne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d333f8",
   "metadata": {},
   "source": [
    "### Batch GD\n",
    "\n",
    "For batch gradient descent, you'll use every instance to calculate the gradient at every step, then use the gradient to update the coefficients.\n",
    "\n",
    "The psuedo-code is in Week 6's lecture! I've provided you a framework where you can fill in the blanks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e718ba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10) #same initial conditions for all\n",
    "\n",
    "eta = 0.0001 # Learning rate, try changing this number!\n",
    "n_iterations = 1000 #Number of iterations, try changing this number!\n",
    "m = 100 #number of points\n",
    "\n",
    "beta_path_bgd = [] #we're going to save the coefficients tested at each iteration in an array, so we can look at the path the gradient descent takes\n",
    "\n",
    "beta = np.random.randn(2,1) # random initial guesses for the coefficients\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = #calculate the gradients for every point. See equation and example above.\n",
    "    beta = # set the new coefficients. See pseudocode in lecture!\n",
    "    beta_path_bgd.append(beta) # add the new coefficients to the array\n",
    "\n",
    "beta_path_bgd = np.array(beta_path_bgd) #save the path\n",
    "\n",
    "beta_bgd = beta #final result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f994e9",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- Compare the final coefficients and MSE loss from the batch gradient descent method to the results from the normal equation method. \n",
    "\n",
    "- What is the percent error on the coefficients from the Batch GD method? In general, does it seem that the slope or intercept is more difficult to determine accurately?\n",
    "\n",
    "- Try increasing the number of iterations, does the error improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa27c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  a code cell to answer the questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e276e6d9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b78e3c3",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "In stochastic gradient descent, a random instance is used to calculate the gradient at each point. Using numpy's $\\texttt{random.randint}$ method, pick a random instance at each iteration (you'll need the feature and target values!) and evaluate the gradient for just that instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e66e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10) #same initial conditions for all\n",
    "\n",
    "eta = 0.000005 # Learning rate, you'll probably need a smaller one than before.\n",
    "n_iterations = # Choose a number of iterations, do you think 1000 will be enough?\n",
    "m = 100 #number of points\n",
    "\n",
    "beta_path_sgd = [] #we're going to save the coefficients tested at each iteration, so we can look at the path the gradient descent takes\n",
    "\n",
    "beta = np.random.randn(2,1) # same initial guesses for the coefficients\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    random_index = #your code to choose a random integer index\n",
    "    x_one = # get the feature values just for that instance\n",
    "    y_one = # get the target values just for that instance\n",
    "    gradients = #calculate the gradient just for that instance. See equation and example above.\n",
    "    beta = # set the new coefficients. See pseudocode in lecture!\n",
    "    beta_path_sgd.append(beta) # add the new coefficients to the array\n",
    "\n",
    "beta_path_sgd = np.array(beta_path_sgd) #save the path\n",
    "\n",
    "beta_sgd = beta #final result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142d545",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "- Compare the final coefficients and MSE loss from the stochastic gradient descent method to the results from the normal equation method. \n",
    "\n",
    "- What is the percent error on the coefficients from the Stochastic GD method? \n",
    "\n",
    "- Try increasing the number of iterations, does the error improve?\n",
    "\n",
    "- What adjustment to the learning rate can you make to get an improvement? Give the percent error on the coefficients after making that adjustment. Explain why this adjustment helped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019c1b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30a33841",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8ada587",
   "metadata": {},
   "source": [
    "### Mini-batch Gradient Descent\n",
    "\n",
    "The \"in-between\" option is Mini-batch Gradient Descent, where you use a small subset of the data (a mini-batch of some size $n$) to calculate the gradient at each iteration. \n",
    "You want to use a random mini-batch for each iteration, so a nice way to implement this is to shuffle the list of indices each time, and then choose the first $n$ indices as your mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f2dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "beta = np.random.randn(2,1) \n",
    "\n",
    "eta = 0.000005\n",
    "\n",
    "n_iterations = 1000\n",
    "\n",
    "m = 100 #number of instances\n",
    "\n",
    "beta_path_mgd = []\n",
    "\n",
    "minibatch_size = 10 #size of the mini batch\n",
    "\n",
    "for epoch in range(n_iterations):\n",
    "    \n",
    "    shuffled_indices = np.random.permutation(m) #shuffle array \n",
    "    \n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    \n",
    "    y_shuffled = yp_wo.reshape(-1,1)[shuffled_indices]\n",
    "    \n",
    "    xi = X_shuffled[:minibatch_size] #select the first set from the shuffled array; equivalent to selecting a random subset\n",
    "    \n",
    "    yi = y_shuffled[:minibatch_size]\n",
    "    \n",
    "    gradients = #gradient for the mini-batch. Make sure you normalize to the right number of points!\n",
    "    \n",
    "    beta = #update the coefficients\n",
    "    \n",
    "    beta_path_mgd.append(beta)\n",
    "\n",
    "beta_path_mgd = np.array(beta_path_mgd)\n",
    "\n",
    "beta_mgd = beta \n",
    "\n",
    "print(beta_mgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eac4b6",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "- Compare the final coefficients and MSE loss from the mini-batch gradient descent method to the results from the normal equation method. \n",
    "\n",
    "- What is the percent error on the coefficients from the Mini-Batch GD method? \n",
    "\n",
    "- What adjustment to the size of the mini-batch can you make to get an improvement? Give the percent error on the coefficients after making that adjustment. Explain why this adjustment helped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00c846e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abf9bf0c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e8e08d5",
   "metadata": {},
   "source": [
    "It's most interesting to actually look at the path taken by GD in the three cases. Increasingly dark colors denote later steps. \n",
    "\n",
    "Note: if you changed the number of iterations and didn't change back to the default values, you may run into plotting errors. You can fix them by adjusting the number of color values in parameters like $\\texttt{c = np.arange(1000)}$ to match your settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5511349",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#plt.text(1.5,3.978,'Normal Equation solution X')\n",
    "plt.scatter(beta_path_sgd[::10, 0].flatten(), beta_path_sgd[::10, 1].flatten(), marker = 's', s = 5, \\\n",
    "         label=\"Stochastic GD, N$_{it}$ = 10000\", c = np.arange(1000), cmap=plt.cm.Purples)\n",
    "plt.scatter(beta_path_mgd[:, 0].flatten(), beta_path_mgd[:, 1].flatten(), marker = \"+\", s = 12, linewidth=1, \\\n",
    "            label=\"Mini-batch GD, N$_{it}$ = 1000\", c = np.arange(1000), cmap=plt.cm.Greens)\n",
    "plt.scatter(beta_path_bgd[:, 0].flatten(), beta_path_bgd[:, 1].flatten(), marker = \"d\", s = 12, linewidth=1, \\\n",
    "            label=\"Batch GD, N$_{it}$ = 1000\", c = np.arange(1000,0,-1), cmap=plt.cm.copper)\n",
    "\n",
    "plt.scatter(beta_sgd[0],beta_sgd[1], marker = \"s\", s = 100, color = 'Purple', alpha = 0.5)\n",
    "plt.scatter(beta_mgd[0],beta_mgd[1], marker = \"+\", s = 200, color = 'DarkGreen', alpha = 1)\n",
    "plt.scatter(beta_bgd[0],beta_bgd[1], marker = \"d\", s = 100, color = 'k', alpha = 0.5)\n",
    "plt.scatter(beta_ne[0],beta_ne[1], marker = \"o\", s = 100, color = 'r', alpha = 0.5, label=\"Normal Equation\")\n",
    "\n",
    "legend = plt.legend(loc=\"upper right\", fontsize=16)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    legend.legend_handles[i].set_color('k')\n",
    "    legend.legend_handles[i]._sizes = [30]\n",
    "\n",
    "\n",
    "plt.xlabel(r\"$\\beta_0$\", fontsize=20)\n",
    "plt.ylabel(r\"$\\beta_1$   \", fontsize=20)\n",
    "\n",
    "plt.axis([1.3, 1.6, 2.5, 6.5])\n",
    "\n",
    "#plt.savefig('AllThePaths.png', dpi = 300)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffcb38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcd5ea88",
   "metadata": {},
   "source": [
    "One way to improve gradient descent methods is by adding an adaptive learning rate: one that changes during the fit. The simplest case is to simply change the value of the learning rate after some number of iterations. Choose one of the gradient descent methods you coded above, and implement it with a learning rate schedule of your choice. Set it up so that it improves the fit (gets it closer to the normal equation result)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e25e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of a gradient descent method with a learning schedule"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PHYS448",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
