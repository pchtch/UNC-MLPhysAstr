{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQQyJRQPZo1v"
   },
   "source": [
    "## In this notebook, you'll use Logistic Regression for the Ising model. \n",
    "\n",
    "It accompanies Chapter 5 of the book (4 of 5).\n",
    "\n",
    "Copyright: Viviana Acquaviva (2023); see also other data credits below.\n",
    "Modifications by Julieta Gruszko (2025)\n",
    "\n",
    "License: [BSD-3-clause](https://opensource.org/license/bsd-3-clause/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgmGOai2Zo1z"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Lw_KF1lZo10"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate, train_test_split\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8avnxktEZo11"
   },
   "source": [
    "### First, let's take a look at those sigmoids!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oHNFMmPUZo11"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-10,10,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzSoUyrRZo12"
   },
   "outputs": [],
   "source": [
    "z = 2*x + 5 #Linear bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_wWGN2tZo13"
   },
   "source": [
    "Let's say that the probability that something will happen is called $\\pi$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_O6zuBvYZo13"
   },
   "source": [
    "The logistic model assumes that\n",
    "\n",
    "$log (\\frac{\\pi}{1-\\pi}$) = z "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjHMQzDvZo13"
   },
   "source": [
    "We can now solve for $\\pi$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVSHLrNRZo15"
   },
   "outputs": [],
   "source": [
    "pi = 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpxD7zkbZo15",
    "outputId": "85247a80-c3ec-4324-8f8e-18a7b71d181a"
   },
   "outputs": [],
   "source": [
    "plt.plot(x, pi)\n",
    "\n",
    "plt.xlim(-7,3);\n",
    "\n",
    "plt.title('Hello, I am a sigmoid!')\n",
    "\n",
    "plt.xlabel('x', fontsize=14)\n",
    "\n",
    "plt.ylabel('$ \\pi$',fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DH0J9PYxZo17"
   },
   "source": [
    "Questions:\n",
    "    \n",
    "- Where does $\\pi$ = 0.5 occur? \n",
    "\n",
    "- What happens if the slope of the linear model is negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrBWd_LTZo17"
   },
   "source": [
    "### We can now see an example from Mehta et al 2018:\n",
    "\n",
    "[\"A high-bias, low-variance introduction to Machine Learning for physicists\"](https://arxiv.org/abs/1803.08823).\n",
    "\n",
    "(Thank you to Pankaj Mehta and David Schwab)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NE4YN33cZo18"
   },
   "source": [
    "We are trying to use a logistic regression model to predict whether a material is in a ordered or disordered phase, based on its spin configuration. In an ordered phase, the spins are aligned. The representation is a 2D lattice so our features are the spin states of each element in the lattice. The physical model, known as Ising model, predicts that the transition depends on temperature and is smeared (for a finite-size lattice), around a critical temperature $T_c$.\n",
    "\n",
    "The training data is composed of 160,000 Monte Carlo simulations in a range of temperatures, and their labels.\n",
    "\n",
    "Possible applications of this formalism involve predicting the critical temperature for more complex systems.\n",
    "\n",
    "Reading in the data might take a little while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6tAsGt1Zo18"
   },
   "outputs": [],
   "source": [
    "#This is gratefully borrowed with permission from the notebooks maintained by P. Mehta.\n",
    "\n",
    "######### LOAD DATA\n",
    "# The data consists of 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25):\n",
    "data_file_name = '../Data/Ising2DFM_reSample_L40_T=All.pkl'\n",
    "# The labels are obtained from the following file:\n",
    "label_file_name = '../Data/Ising2DFM_reSample_L40_T=All_labels.pkl'\n",
    "\n",
    "\n",
    "#DATA\n",
    "with open(data_file_name, 'rb') as pickle_file:\n",
    "    data = pickle.load(pickle_file) # pickle reads the file and returns the Python object (1D array, compressed bits)\n",
    "\n",
    "data = np.unpackbits(data).reshape(-1, 1600) # Decompress array and reshape for convenience\n",
    "data=data.astype('int')\n",
    "data[np.where(data==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
    "\n",
    "#LABELS (convention is 1 for ordered states and 0 for disordered states)\n",
    "with open(label_file_name, 'rb') as pickle_file:\n",
    "    labels = pickle.load(pickle_file) # pickle reads the file and returns the Python object (here just a 1D array with the binary labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZ2xLL-cZo19",
    "outputId": "1fe9c399-739a-4a40-8ea6-0019017c2e80"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0MJrqE4Zo19",
    "outputId": "3ad59a49-6b26-4da5-9fe7-9db1f9d76456"
   },
   "outputs": [],
   "source": [
    "np.unique(labels)\n",
    "#labels: 1 = ordered or near-critical\n",
    "#labels: 0 = disordered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49GjVjxuZo1-"
   },
   "source": [
    "Check the label distribution. Are the classes balanced or imbalanced? Do the data need to be shuffled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuBU35B4Zo2A"
   },
   "source": [
    "#### We can take a look at a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppDWhqS3Zo2A",
    "outputId": "f76a9f97-a5fd-4eba-f1bc-62129bfe785c"
   },
   "outputs": [],
   "source": [
    "#H/T: https://stackoverflow.com/questions/16834861/create-own-colormap-using-matplotlib-and-plot-color-scale\n",
    "\n",
    "cmap = matplotlib.colors.ListedColormap([\"aquamarine\",\"navy\"], name='from_list', N=None)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "fig, axarr = plt.subplots(nrows=1, ncols=3)\n",
    "axarr[0].imshow(data[0].reshape(40,40), cmap = cmap) #first object has label \"1\"\n",
    "axarr[1].imshow(data[80000].reshape(40,40), cmap = cmap) #from documentation, this is critical-ish (between 60, and 90,000)\n",
    "axarr[2].imshow(data[100000].reshape(40,40), cmap = cmap) #disordered\n",
    "for i in range(3):\n",
    "    axarr[i].set_xticks([0,20,40]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrDo7UeBZo2A"
   },
   "source": [
    "### Let's pick a random selection to speed up the computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBh9eoKBZo2B"
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "sel = np.random.choice(data.shape[0], 16000, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TaKrAcc4Zo2B"
   },
   "outputs": [],
   "source": [
    "seldata = data[sel,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0mtx8cUZo2B"
   },
   "outputs": [],
   "source": [
    "sellabels = labels[sel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKvQmS4KZo2C",
    "outputId": "45935905-e1e6-45bb-da50-5401dbd1614b"
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(seldata.shape[0]),sellabels); #The random selection also has the advantage of reshuffling the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many features are we using (this is our largest feature space yet!)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taicNCHTZo2C"
   },
   "source": [
    "### And now time for the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kYE-PYODZo2C"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter = 1000) #This uses a numerical method to find the minimum of the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_5jJ-rIZo2C",
    "outputId": "8a4daf18-bc9c-4de1-d403-b0b8744ec1e0"
   },
   "outputs": [],
   "source": [
    "model.get_params() #Note that (unlike in linear regression) regularization is the norm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iP_kBiHQZo2D",
    "outputId": "07a5bdc1-5833-446f-86f5-bba1e96f90d6"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1NbAb-UZo2D"
   },
   "source": [
    "Using cross validation, as usual, train the model and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to train model and get results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What metric is being reported? Is this enough information? What other information might we want?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Vi0wSNaZo2F"
   },
   "source": [
    "### Do your own grid search to optimize the regularization parameter C. \n",
    "\n",
    "Check log-spaced values of C between 1E-3 and 1E3 (in other words, C = {1E-3, 1E-2, ..., 1E3}). No need to for cross-validation this time around; think of this as a preliminary exploratory phase, not reporting our final results.\n",
    "\n",
    "Note that our data is already very regular (feature values are -1/1), so we are not doing any scaling.\n",
    "\n",
    "Does regularization make a noticeable improvement to this model's performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMWzq8l-Zo2F",
    "outputId": "c704ce7e-7f6e-4545-c47c-b19e7a980325"
   },
   "outputs": [],
   "source": [
    "#Test each value of C and report the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7utkqo_Zo2G"
   },
   "source": [
    "### Now let's generate labels in order to check predictions.\n",
    "\n",
    "For those classifiers that are solving a regression problem under the hood, there is the handy \"predict_proba\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xUkJl1JSZo2H"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=1.0, max_iter=1000)\n",
    "\n",
    "ypred = cross_val_predict(model, seldata, sellabels, \\\n",
    "                               cv = KFold(n_splits=5, shuffle=True, random_state=10))\n",
    "\n",
    "ypred_prob = cross_val_predict(model, seldata, sellabels, \\\n",
    "                               cv = KFold(n_splits=5, shuffle=True, random_state=10), method = 'predict_proba')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYzss_R5Zo2H"
   },
   "source": [
    "The output of predict_proba gives the probability to belong to disordered (label 0) or ordered (label 1) phase, in that order (these add to 1 of course, as they should). The simple classifier output is the class with p > 0.5. We can look at this to convince ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VK3_rHVVZo2H",
    "outputId": "b5f8b14f-c1b2-4e18-bf38-417d9ba28023"
   },
   "outputs": [],
   "source": [
    "np.column_stack([ypred_prob, ypred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a ROC curve to check the performance\n",
    "Is 0.5 really the best threshold to set? Maybe or maybe not! It depends on your application. We can get a more complete picture of the performance using an ROC curve. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(sellabels, ypred_prob[:, 1])\n",
    "\n",
    "plt.plot(fpr, tpr, label='Logistic Regression ROC Curve')\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjLRxlWoZo2H"
   },
   "source": [
    "### We can plot a few examples to see how our classifier is doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDPNCNxSZo2I",
    "outputId": "362c3603-2c27-4dfa-c651-80b8439b972c"
   },
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(nrows=1, ncols=8, figsize=(15,5))\n",
    "for i in range(8):\n",
    "    axarr[i].imshow(seldata[i].reshape(40,40), cmap = cmap) \n",
    "    axarr[i].set_xlabel('True label:'+str(sellabels[i])+'\\n'+'Pred label:'+str(ypred[i]))\n",
    "    axarr[i].set_yticks([])\n",
    "    axarr[i].set_xticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5P0Exe_jZo2I"
   },
   "source": [
    "Unfortunately, there are two instances that are misclassified by our Logistic Regressor classifier. At least visually, this is understandable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnHvxm_JZo2I"
   },
   "source": [
    "Let's take a look at the corresponding probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pZA2DE1Zo2J",
    "outputId": "4897d390-485f-40ce-b464-583195b0968a"
   },
   "outputs": [],
   "source": [
    "ypred_prob[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How confident is the model about its choice for the first 8 instances? Does this raise any concerns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJwovq9SZo2K"
   },
   "source": [
    "### Some analysis:\n",
    "The conclusion is that the main indicator for this model is lack of consistency between spin alignments, which is not modeled well by our regressor. It's a tricky problem because many algorithms tend to look at the value of each feature to decide - for many of them, it's hard to represent the correlation among features as an indicator. \n",
    "\n",
    "\n",
    "### Improving the model:\n",
    "One way to improve the performance may be to add engineered features that combine the behavior of neighboring spins. Pooling is one approach to do this: you add new features that are calculated by reducing the dimensionality of the lattice from 40x40 to 20x20 by combining adjacent cells and rounding the average spin (e.g. a cell with 4 pixels with spins -1, 1, 1, 1 would be assigned 1, a cell with spins -1, -1, 1, 1 would be assigned 0, a cell with -1, -1, -1, 1 would be assigned -1). \n",
    "\n",
    "Add 4-pixel pooling features to the data, and then repeat fitting with linear regression. Does this give an improved result?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here an example of selecting from a data point, just to help you figure out how to do this\n",
    "inst = seldata[3].reshape(40, 40)\n",
    "inst[0:2, 0:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a framework for your pooling steps. Save the result as a new set of features for each instance.\n",
    "# Note, this is a very slow way to do this! On my laptop, it took about 30 seconds to run. \n",
    "# It's not very pythonic to have all those loops, and we pay the cost in processing time since the code isn't optimizing the processes at all.\n",
    "# On the other hand, it's nice and easy to understand. \n",
    "# If you want to write something that runs more quickly, this will help: https://numpy.org/devdocs/reference/generated/numpy.lib.stride_tricks.sliding_window_view.html\n",
    "pooledfeat = []\n",
    "for inst in range(seldata.shape[0]):\n",
    "    inst = seldata[inst].reshape(40, 40)\n",
    "    pooledinst = []\n",
    "    for i in range(0, inst.shape[0], 2):\n",
    "        for j in range(0, inst.shape[1], 2):\n",
    "            #average and round the values in 4 pixels, append the results to pooledinst\n",
    "    #append each pooledinst to pooledfeat\n",
    "\n",
    "# the feature array comes out with some nearly-but-not-quite 0 values because of numerical precision issues, this fixes that problem\n",
    "pooledfeat = np.where(np.array(pooledfeat)<1E-6, 0, pooledfeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the new features to the old ones as new columns using np.concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train logistic regression again with your previous best value of C and check the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did the model perform with the added features? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the ROC curve for the new model, comparing it to your original model. What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PHYS448",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
