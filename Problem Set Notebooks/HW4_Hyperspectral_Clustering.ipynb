{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you'll work on clustering hyperspectral image data, using dimensionality reduction to help guide your work. \n",
    "\n",
    "It accompanies Chapter 7 of the book.\n",
    "\n",
    "Copyright: Viviana Acquaviva (2023).\n",
    "\n",
    "Additions and modifications by Julieta Gruszko (2025).\n",
    "\n",
    "License: [BSD-3-clause](https://opensource.org/license/bsd-3-clause/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperspectral image data\n",
    "\n",
    "Hyperspectral images are data that contain both spatial (position x, y) and spectral (wavelength $\\lambda$) data. For each pixel in an image, information about the spectral emission on that pixel is corrected, as shown in this image:\n",
    " ![A set of stones is scanned with a Specim LWIR-C imager in the thermal infrared range from 7.7 μm to 12.4 μm. The quartz and feldspar spectra are clearly recognizable.](./HSI_LWIR_stones.png)\n",
    "\n",
    " Image Credit: Holma, H., (May 2011), Thermische Hyperspektralbildgebung im langwelligen Infrarot, Photonik\n",
    "\n",
    "\n",
    "\n",
    "Hyperspectral imaging is useful, for example, in aerial surveys, to identify different types of terrain based on their spectral signature and location. Of course, usual color photos already contain spectral information, since they tell you the average emission in the red, green, and blue (RGB) parts of the spectrum. But the frequency bands used in that case are very wide; what we call hyperspectral images contain many more frequency bands that are much more narrow. \n",
    "\n",
    "Due to this difference, the way we treat them in this clustering example is different: instead of treating each image as an instance, and each pixel of the image (or maybe the red, green, and blue versions of each pixel) as a feature, as we did in our galaxy classification task, we'll treat each pixel as an instance, and the flux at each frequency as a feature. We'll only be looking at a single image, and we'll seek to cluster/classify the pixels of the image. \n",
    "\n",
    "In this homework, we'll work with a \"classic\" data set: aerial images of the city of Pavia, Italy, collected by the European Reflective Optics System Imaging Spectrometer (ROSIS-3) satellite. We'll use the \"Pavia Center\" data set, which consists of one composite image of 1,096 x 715 pixels, with a 1.3m spatial resolution. Each pixel is measured in 102 spectral bands between 430 and 834nm. \n",
    "\n",
    "Our goal is to figure out what type of land-cover each pixel of the image represents. Some of the pixels are labeled, which will let us check our work, but our goal is to do this task using unsupervised learning tools. So you know what we're working with, the labels and frequencies in the data set are:\n",
    "\n",
    "| Label | Class | Membership |\n",
    "|:---------|:--------:|---------:|\n",
    "| 0 | Unlabeled | 635488 |\n",
    "| 1 | Water | 65971 |\n",
    "| 2 | Trees | 7598 |\n",
    "| 3 | Asphalt | 3090 |\n",
    "| 4 | Self-blocking Bricks | 2685 |\n",
    "| 5 | Bitumen | 6584 |\n",
    "| 6 | Tiles | 9248 |\n",
    "| 7 | Shadows | 7287 |\n",
    "| 8 | Meadows| 42826 |\n",
    "| 9 | Bare Soil | 2863 |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib\n",
    "\n",
    "font = {'size'   : 16}\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14) \n",
    "matplotlib.rcParams.update({'figure.autolayout': False})\n",
    "matplotlib.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, adjusted_rand_score\n",
    "from sklearn.metrics import pairwise\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Pavia centre data set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we read the data (an image of the center of Pavia, in Italy) and the ground truth labels that assign a classification to each of the pixels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the uncompressed Pavia center data set is a bit too big to be stored using Github's normal file distribution system. Instead, the data available on the Canvas site. They're linked in the \"Problem Sets\" module. You'll need to download them and point the commands below to the correct directory on your computer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('/path/to/Pavia.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'pavia'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1096, 715, 102)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat['pavia'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mat['pavia']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: \n",
    "Describe the structure and contents of the file that you just loaded, and explain how you'll use it in your clustering algorithm, assuming we'll be following the approach described above for hyperspectral images. Include a description of everything in the file, not just what we've called the $\\texttt{data}$ object, but make sure you also describe how the $\\texttt{data}$ object is structured and what it contains. \n",
    "\n",
    "How many instances and how many features will we be using?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same approach as above, load the $\\texttt{Pavia\\_gt.mat}$ file, check the keys contained in it, and save the ground truth labels to a new data structure $\\texttt{labels}$. Check the shape of the labels and the unique values found in it. Then, using the table above, make a dictionary that connects each numerical value to the correct descriptive text label. The keys should be the integer labels, and the values should be the text labels. If you don't have any experience with Python dictionaries, you can find some documentation here: https://docs.python.org/3/tutorial/datastructures.html#dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to load the file and check the contain keys\n",
    "labels = # store the ground truth labels here\n",
    "# then check the shape and unique values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_names = # make a dictionary to map integer labels to the descriptive text labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: \n",
    "Describe the structure and contents of the file that you just loaded. Include a description of everything in the file, not just what we've called the $\\texttt{labels}$ object, but make sure you also describe how the $\\texttt{labels}$ object is structured and what it contains. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize map and ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the existing literature to reproduce a false color composite RGB (bands 10, 27, 46):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(data[:,:,10]+data[:,:,27]+data[:,:,46])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,20))\n",
    "plt.imshow(data[:,:,10]+data[:,:,27]+data[:,:,46], interpolation='nearest', cmap='gist_gray', vmax = 10000)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('PaviaCentreMap.pdf', dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may be able to tell, this is a composite image made up of two aerial images stuck together. We can also show the ground truth labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,12))\n",
    "plt.imshow(labels, interpolation='nearest', cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at label distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(np.unique(labels.flatten()))):\n",
    "    print(i, len(np.where(labels.flatten() == i)[0]), gt_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_names.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because the number 0 is not associated to a class, we will get rid of pixels with label = 0.\n",
    "\n",
    "Next, we'll create the feature matrix, using a mask to drop any unlabeled pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (labels != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of pixels with label = 0 for the clustering, and mask the labels the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskdata = data[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskdata.shape # ~148,000 instances (pixels), 102 attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "masklabels = labels[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, scale the data before clustering.\n",
    "\n",
    "We'll want the scaling we apply to work well for k-means and PCA, since that's what we'll be doing to start with. What properties should it have? Explain your answer, and then scale the data using an appropriate sk-learn scaler. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskdata_scaled = # scale your data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run clustering using k-means and the known number of labels. \n",
    "\n",
    "This isn't stricly unsupervised learning since we're using the known number of classes; later we'll change this.\n",
    "\n",
    "Carefully consider your initialization strategy and number of initializations! \n",
    "\n",
    "Get the cluster assignments for all the data points using $\\texttt{fit\\_predict}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to run k-means clustering on your scaled data and get the cluster assignments for all points.\n",
    "clusters = # save the assigned clusters here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the labels used run from 0 to 8, we'll adjust them to run 1-9, like our truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = clusters + 1 # rename labels to be between 1 and 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the distribution of objects in clusters to get a general idea of whether the clustering was successful. I've started you off with a plot of the ground truth labels, just add a histogram of your cluster labels on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "plt.hist(masklabels.flatten(), alpha = 0.5, color = 'm', label = 'True', bins = 9, range = (0.5,9.5))\n",
    "# plot an overlapping histogram of your cluster results\n",
    "plt.title('Clustering results by $k$-means, $k$ = 9', fontsize=16)\n",
    "plt.legend(fontsize = 16);\n",
    "plt.xlabel('Class or cluster ID', fontsize = 16);\n",
    "plt.ylabel('Number of samples', fontsize = 16);\n",
    "plt.xticks(np.arange(1,10));\n",
    "#plt.savefig('kmeans9prediction.pdf', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the indexing is not significant (clusters are assigned at random)!\n",
    "\n",
    "### Question:\n",
    "What can you guess about your algorithm's success in clustering the most common and second most common categories, from these statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted Rand Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Adjusted Rand Index gives an idea of how much two distributions are similar, and can be used to compare two clustering schemes. It varies between 0 and 1, with 0 being \"equivalent to random\" and 1 being perfectly equivalent. We get a value of ~0.77, not bad, although as we learned with any measure of similarity, there is no immediate interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_kmeans = adjusted_rand_score(masklabels.flatten(), clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there is no \"absolute interpretation\" of the ARI value, a value of 0.77 indicates that the original distribution and the one found by k-means are in reasonable agreement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find centroids from ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful exercise for this problem is to calculate \"accuracy\" by assigning labels to predicted clusters (note that in most clustering problems, ground truth labels are not available - this is a special case!) \n",
    "\n",
    "For this, we need to map each cluster found by k-means to one of the ground truth labels. This is non-trivial, but we will attempt to do it by identifying each \"predicted\" centroid with the closest ground truth centroid.\n",
    "\n",
    "We start by calculating the ground truth centroids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_centroids = []\n",
    "\n",
    "for i in range(1, len(np.unique(masklabels.flatten()))+1): #remember that labels go from 1 to 9\n",
    "    indices = (np.where(masklabels.flatten() == i)[0])\n",
    "    gt_centroids.append(np.mean(maskdata_scaled[indices,:], axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we calculate the matrix of distances, in feature space, between ground-truth-based (GT) centroids and clustering-based (CB) centroids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = pairwise.euclidean_distances(gt_centroids, kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the matrix\n",
    "\n",
    "#From here: https://stackoverflow.com/questions/32503308/plot-distance-matrix-for-a-1d-array\n",
    "    \n",
    "plt.figure(figsize = (20,20))\n",
    "\n",
    "plt.matshow(C,cmap=\"Blues\",fignum=1)\n",
    "\n",
    "n = int(np.sqrt(C.size))\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "# Set the plot labels\n",
    "xlabels = [\"CB%d\" % i for i in range(n+1)]\n",
    "ylabels = [\"GT%d\" % i for i in range(n+1)]\n",
    "#print(xlabels)\n",
    "ax.set_xticklabels(xlabels)\n",
    "ax.set_yticklabels(ylabels)\n",
    "\n",
    "#Add text to the plot showing the values at that point\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        plt.text(j,i, np.round(C[i,j],2), horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now map each clustering-based centroid to the closest GT centroid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find closest GT centroid for each CB centroid; assign corresponding label \n",
    "\n",
    "for i in range(kmeans.n_clusters):\n",
    "    print(i+1, np.argmin(C[:,i])+1, np.round(C[np.argmin(C[:,i]),i],4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this does not guarantee a 1:1 mapping! Some ground truth labels may not be getting assigned to any k-means cluster, and multiple k-means clusters may be associated with the same ground truth label.\n",
    "\n",
    "Additionally, we can see that the distances to closest centroids vary quite a bit; we will use visualization techniques in a moment to see if we can interpret this behavior as less separated/identifiable clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "What ground truth labels don't seem to be assigned to any k-means cluster, if any?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary to re-assign the predicted cluster membership, so we can calculate label-based metrics, such as accuracy. The mapping should go from current predicted label (as the key) to GT label (as the value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = # your remapping dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final complication is that we also have the background pixels (with label 0), which will be part of the final predicted image (but not participate in the accuracy calculation, of course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.where(labels.flatten() != 0)[0] #these are the non-background pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predlabels = np.zeros(len(labels.flatten())) #start with all-background labels of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predlabels[indices] = clusters #assign predicted cluster membership where label is not 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predlabels_remap = np.copy(predlabels)\n",
    "\n",
    "for k, v in d.items(): predlabels_remap[predlabels == k] = v #re-map labels according to the new labeling scheme we created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check manually that the re-labeling worked as expected. Make sure the outputs here match what you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predlabels[200:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predlabels_remap[200:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finally ready to calculate the accuracy score. Calculate it using the original labels and remapped labels, using sk-learn's $\\texttt{accuracy\\_score}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to get accuracy score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this too good to be true? Certainly so, because we should remember to exclude background pixels (which are, in fact, the majority)! Try again, using only the indices corresponding to non-background pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to get accuracy score of non-background pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "What is the accuracy score for the labeled (non-background) pixels, using 9 k-means clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the score you see shouldn't be too bad - because we know one class comprises almost half the object, we can also invoke a full confusion matrix or consider a class-by-class accuracy. Using sk-learn's $\\texttt{confusion\\_matrix}$ option from $\\texttt{metrics}$, make a confusion matrix showing percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2, suppress=True) #Suppress scientific notation\n",
    "\n",
    "# make the confusion matrix and display it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can display the GT and predicted images (after making sure that the color scale is the same - here it's not trivial, because the predicted labels only vary between 1 and 8):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1,2,figsize=(15,10.5));\n",
    "\n",
    "# Add legend mapping colors to classes\n",
    "norm = matplotlib.colors.Normalize(vmin=0, vmax=9)\n",
    "for i in range(1, kmeans.n_clusters+1):\n",
    "    rgba_color = cm.viridis(norm(i)) \n",
    "    plt.scatter(0,0, s = 20, c = np.atleast_2d(rgba_color), label = gt_names[i], marker='s')\n",
    "plt.legend(loc=[1.1,0.535], markerscale=3, fontsize=18)\n",
    "\n",
    "# Plot ground truth vs predictions\n",
    "axarr[0].imshow(labels, interpolation='None', cmap='viridis', vmin = 0, vmax = 9);\n",
    "axarr[0].set_title('True', fontsize = 16)\n",
    "pl = axarr[1].imshow(predlabels_remap.reshape(1096, 715), interpolation='None', cmap='viridis', vmin = 0, vmax = 9);\n",
    "axarr[1].set_title('Predicted ($k$-means, $k$ = 9)',fontsize = 16);\n",
    "plt.tight_layout()\n",
    "#plt.savefig('GTvskmeans9.pdf', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- What differences do you observe? You may have to look carefully. Identify at least two differences.\n",
    "\n",
    "- What categories is our clustering algorithm capturing well? Which ones is it capturing poorly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize \"incorrect\" assignments below by creating a 0/1 mask where 0 are correctly assigned pixels, and 1 and incorrectly assigned pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormask = np.array([labels.flatten() == predlabels_remap]).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1,2,figsize=(15,15));\n",
    "axarr[0].imshow(labels, interpolation='None', cmap='viridis');\n",
    "axarr[0].set_title('True', fontsize = 16)\n",
    "pl = axarr[1].imshow(colormask.reshape(1096, 715), interpolation='None', cmap = 'gray')#;\n",
    "axarr[1].set_title('Incorrect assignments', fontsize = 16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next step: Visualization with PCAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the hyperspectral signature of some random pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "for i in np.random.randint(0,42000,10):\n",
    "    plt.plot(range(102),maskdata_scaled[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When no \"spiky\" features are present, we can expect a decent performance from linear PCAs. \n",
    "\n",
    "Apply a linear PCA to your scaled, masked data. \n",
    "\n",
    "Then, make a plot of the cumulative explained variance as a function of the number of components, as we did in Studio 11. As we did there, you don't need to include all of the possible PCA components in the plot, but make sure you include enough to see where the improvement flattens out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = # our PCA model \n",
    "# code to apply linear PCA and make a plot of cumulative explained variance. Make sure you label your axes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "- How many components are needed to encode 99% of the variance? How much of the variance is encoded in just the first two components?\n",
    "\n",
    "- Based on your answer, should we expect a 2-D visualization of the data to be useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll plot data on the 2D PCA projections, just to get an idea of separability. I've given you the code to set up the color scale,  show the ground truth labels, and label the axes. You'll need to add the code to plot the transformed data, color-coding it by the ground truth label. Look back at the Studio 11 code if you need a reminder on how to transform the data. \n",
    "\n",
    "Hint: use $\\texttt{c = masklabels}$ in your plotting command to set the point color using the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,10))\n",
    "\n",
    "# Add legend mapping colors to classes\n",
    "norm = matplotlib.colors.Normalize(vmin=0, vmax=9)\n",
    "rgba_colors = [cm.viridis(norm(i)) for i in range(1, kmeans.n_clusters+1)]\n",
    "   \n",
    "# add your code to make the scatter plot here\n",
    "\n",
    "\n",
    "\n",
    "plt.text(transformer.transform(np.array(gt_centroids))[0,0],\\\n",
    "            transformer.transform(np.array(gt_centroids))[0,1]+0.9,str(gt_names[1]), fontsize = 16) #Add \"water\" \n",
    "                                                                                                    #label separately\n",
    "    \n",
    "for i in range(2, kmeans.n_clusters+1):\n",
    "    plt.text(transformer.transform(np.array(gt_centroids))[i-1,0]+0.1,\\\n",
    "             transformer.transform(np.array(gt_centroids))[i-1,1]+0.3,str(gt_names[i]), fontsize = 16)\n",
    "\n",
    "\n",
    "plt.xlabel('PCA 1', fontsize =16)\n",
    "plt.ylabel('PCA 2', fontsize =16)      \n",
    "plt.xlim(-10,25)\n",
    "plt.ylim(-12,10);\n",
    "\n",
    "#plt.savefig('PCAmap.pdf', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "Using the PCA plot, interpret the results of the k-means clustering. Can you explain why the clustering performed well on the groups that it did, and poorly on the groups that it did?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also understand more the mapping between clusters are classes by plotting the ground truth + predicted centroids. Again, I've given you most of the setup, but you'll need to add scatter plots of the transformed ground truth and k-means centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,10))\n",
    "\n",
    "# Add legend mapping colors to classes\n",
    "norm = matplotlib.colors.Normalize(vmin=0, vmax=9)\n",
    "rgba_colors = [cm.viridis(norm(i)) for i in range(1, kmeans.n_clusters+1)]\n",
    "   \n",
    "\n",
    "# scatter plot of transformed ground truth centroids\n",
    "\n",
    "# scatter plot of transformed k-means centroids\n",
    "\n",
    "\n",
    "for i in range(1, kmeans.n_clusters+1):\n",
    "    plt.text(transformer.transform(np.array(gt_centroids))[i-1,0]+0.1,\\\n",
    "             transformer.transform(np.array(gt_centroids))[i-1,1]+0.3,str(gt_names[i]), fontsize = 16)\n",
    "    plt.text(transformer.transform(np.array(kmeans.cluster_centers_))[i-1,0]-0.1,\\\n",
    "             transformer.transform(np.array(kmeans.cluster_centers_))[i-1,1]-0.5,str(i), fontsize = 16)\n",
    "plt.xlabel('PCA 1', fontsize =16)\n",
    "plt.ylabel('PCA 2', fontsize =16)      \n",
    "\n",
    "plt.legend(fontsize =16);\n",
    "\n",
    "#plt.savefig('PCAcentroids.pdf', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- Of the 9 clusters you asked k-means to create, which seem to correspond to ground truth clusters, based on the PCA analysis? \n",
    "- Are there any k-means clusters you'd suggest merging to better align with the ground truth labels? If so, describe which you'd merge, and what ground truth label they correspond to. This type of post-processing \"cluster merging\" is pretty common when you're using unsupervised methods for science. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last question that we could ask is: if we had treated this problem as a classic unsupervised problem, where the number of clusters is not known a priori, what would have we found?\n",
    "\n",
    "We can try using a few tools that we have learned about. The simplest choice for this case is probably the Elbow method; the silhouette score can also work, but because it requires calculating pairwise distances among all objects, it would be prohibitive to calculate it on the full data set (we could solve this with random sampling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "Based on the PCA analysis, should we expect the elbow method to perform reasonably well on this data? Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an elbow method plot for this data, using k-means. You'll need to decide for yourself what values you want to sample for numbers of clusters. Go back to Studio 10 if you need a reminder on how to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code to get the intertias for varying numbers of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code to make an elbow curve plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your elbow curve, pick a preferred number of clusters and re-run the k-means clustering. Then check the histogram of class distributions, the adjusted Rand score, and the accuracy on non-background pixels, using the same strategy we used above to map the cluster labels to appropriate ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to re-run k-means\n",
    "# code to make the histogram comparison to ground truth labels\n",
    "# code to determine labels using distances and re-assign labels with a dictionary\n",
    "# code to determine accuracy on non-background pixels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can visualize the map of ground truth labels vs predictions (you may need to change variable names to get this code to run on your results!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1,2,figsize=(15,15));\n",
    "\n",
    "# Add legend mapping colors to classes\n",
    "norm = matplotlib.colors.Normalize(vmin=0, vmax=9)\n",
    "for i in range(1, kmeans5.n_clusters+1):\n",
    "    rgba_color = cm.viridis(norm(i)) \n",
    "    plt.scatter(0,0, s = 20, c = np.atleast_2d(rgba_color), label = gt_names[i], marker='s')\n",
    "plt.legend(loc=[1.1,0.57], markerscale=3, fontsize=18)\n",
    "\n",
    "# Plot ground truth vs predictions\n",
    "axarr[0].imshow(labels, interpolation='None', cmap='viridis', vmin = 0, vmax = 9);\n",
    "axarr[0].set_title('True', fontsize = 16)\n",
    "pl = axarr[1].imshow(predlabels5_remap.reshape(1096, 715), interpolation='None', cmap='viridis', vmin = 0, vmax = 9);\n",
    "axarr[1].set_title('Predicted, k-Means, k = 5',fontsize = 16);\n",
    "#plt.colorbar(pl,ax=axarr[:], shrink = 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "How does the accuracy using your chosen number of clusters compare with the accuracy using 9 clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our final experiment, we use Gaussian Mixture Models to see if they are better at picking up the correct number of clusters and at generating a clustering scheme.\n",
    "\n",
    "As in Studio 10, use the BIC to decide how many components to use. It's up to you to decide whether you should allow the GMM to use full covariances, or if you think we should restrict them. Warning: this can be slow, particularly if you decide to study many models with full covariances! You'll need to balance the need to get useful information with computational feasibility.\n",
    "\n",
    "Then cluster the data with however many components your BIC indicates is optimal; check the histogram of class distributions, the adjusted Rand score, and the accuracy on non-background pixels, using the same strategy we used above to map the cluster labels to appropriate ground truth labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- How does the GMM performance compare with both examples of the k-means performance? \n",
    "- Based on the label frequency histogram, how is GMM performing on the 2 most-frequent classes?\n",
    "- Using the PCA analysis, explain why GMM performs as it does on this data, compared with k-means. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can visualize the map of ground truth labels vs predictions (you may need to change variable names to get this code to run on your results!). If you're interested, you can also repeat the PCA centroid analysis for the GMM, but you're not required to!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1,2,figsize=(15,10.5));\n",
    "\n",
    "# Add legend mapping colors to classes\n",
    "norm = matplotlib.colors.Normalize(vmin=0, vmax=9)\n",
    "for i in range(1, len(np.unique(labels))):\n",
    "    rgba_color = cm.viridis(norm(i)) \n",
    "    plt.scatter(0,0, s = 20, c = np.atleast_2d(rgba_color), label = gt_names[i], marker='s')\n",
    "plt.legend(loc=[1.1,0.535], markerscale=3, fontsize=18)\n",
    "\n",
    "# Plot ground truth vs predictions\n",
    "axarr[0].imshow(labels, interpolation='None', cmap='viridis', vmin = 0, vmax = 9);\n",
    "axarr[0].set_title('True', fontsize = 16)\n",
    "pl = axarr[1].imshow(predlabelsGMM_remap.reshape(1096, 715), interpolation='None', cmap='viridis', vmin = 0, vmax = 9);\n",
    "axarr[1].set_title('Predicted (GMM, n$_{comp}$ = 8)',fontsize = 16);\n",
    "plt.tight_layout()\n",
    "#plt.savefig('GTvsGMM8.pdf', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite interesting to note that in the GMM map, there is very little \"salt and pepper\" noise - classes may be wrong, but they are wrong \"in blocks\", even without using spatial information. We can visualize this behavior by taking a look at the incorrect pixel map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormask = np.array([labels.flatten() == predlabelsGMM_remap]).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1,2,figsize=(15,15));\n",
    "axarr[0].imshow(labels, interpolation='None', cmap='viridis');\n",
    "axarr[0].set_title('True', fontsize = 16)\n",
    "pl = axarr[1].imshow(colormask.reshape(1096, 715), interpolation='None', cmap = 'gray')#;\n",
    "axarr[1].set_title('Incorrect assignments', fontsize = 16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "Data: http://www.ehu.eus/ccwintco/index.php?title=Hyperspectral_Remote_Sensing_Scenes#Pavia_University_scene\n",
    "\n",
    "Classification: https://arxiv.org/pdf/1612.00144v2.pdf reports kNN accuracy on scaled data set of 77% on 9 classes\n",
    "\n",
    "Ref [80] in master thesis: https://arxiv.org/pdf/1704.07961.pdf (bunch of clustering results on 6-class data set)\n",
    "\n",
    "Master's thesis: https://www.ri.cmu.edu/wp-content/uploads/2020/05/Masters_thesis_2020-1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgement Statement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PHYS448",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
