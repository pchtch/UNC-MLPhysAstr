{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-hdsitToEwV"
   },
   "source": [
    "This is a simple notebook that gives a demonstration of k-means, DBSCAN and GMM.\n",
    "\n",
    "It accompanies Chapter 7 of the book and also shows how different figures were made.\n",
    "\n",
    "Copyright: Viviana Acquaviva (2023); see also other credits below, in particular Jake Vanderplas' [excellent notebooks](https://jakevdp.github.io/PythonDataScienceHandbook/05.00-machine-learning.html).\n",
    "\n",
    "Modifications by Julieta Gruszko (2025)\n",
    "\n",
    "License: [BSD-3-clause](https://opensource.org/license/bsd-3-clause/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2179,
     "status": "ok",
     "timestamp": 1685198207018,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "8x0c1CRfoEwV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a new library, mlxtend, to plot decision regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VUWWvu-3oblf"
   },
   "outputs": [],
   "source": [
    "%pip install mlxtend --upgrade # I had to run this to make plot_decision_regions work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1183,
     "status": "ok",
     "timestamp": 1685198244796,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "vggBlAHYoEwW"
   },
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll make an example data set to try out our clustering approaches, and plot them to take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "executionInfo": {
     "elapsed": 2366,
     "status": "ok",
     "timestamp": 1685198249586,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "LqYzFlcxoEwX",
    "outputId": "987f1a1f-f350-4455-d6d8-6af297b9f3d7"
   },
   "outputs": [],
   "source": [
    "X, y_true = make_blobs(n_samples=300, centers=4,\n",
    "                       cluster_std=0.6, random_state=2)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s =3, c ='gray') # plot original points\n",
    "plt.xlim(-10,5);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "- Describe what $\\texttt{make\\_blobs}$ has generated as our fake data. Of the 3 clustering techniques we discussed (k-means, DBSCAN, GMM), which do you expect to perform well on this data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll try clustering the data with k-means. For now we'll initialize the starting points by hand, just to make sure results are reproducible. We'll plot the results of k-means clustering after convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "start = np.array([[-1,1],[1,-1],[3,-3],[-5,-10]]) #initial points (fixed for reproducibility; Fig 7.3)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X[:, 0], X[:, 1], s =3, c ='gray') # plot original points\n",
    "plt.xlim(-10,5);\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "kmeans = KMeans(n_clusters=4, init = \"random\", n_init=1)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(X[:, 0], X[:, 1], s = 3, c = y_kmeans, cmap = 'rainbow') # plot original points\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=20, edgecolor = 'k', c = [0,1,2,3],cmap = 'rainbow');\n",
    "plt.xlim(-10,5);\n",
    "plt.annotate('Converged', xy=(77, 20), xycoords='axes points', size=14, ha='right', va='top')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans.get_params()) # as usual, we can see the parameters of the model\n",
    "print(kmeans.n_iter_) #we can also print attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "- How many iterations did it take for k-means to converge, in this case? How does that compare to the maximum allowed number of iterations?\n",
    "- How did we initialize the k-means algorithm, in this case? Change the initialization strategy. Do the results or required number of iterations change?\n",
    "- What does the $\\texttt{n\\_init}$ parameter control? When might you want to set it to something other than 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we knew the correct number of clusters becayse we generated the data, but we can use the elbow curve plot to see when the benefit of adding additional clusters \"levels off\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertiasb = []\n",
    "for k in range(2, 10):\n",
    "    kmeans = KMeans(n_clusters=k, n_init = 10)\n",
    "    kmeans.fit(Xb)\n",
    "    inertiasb.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(2, 10), inertiasb)\n",
    "#plt.grid(True)\n",
    "plt.title('Elbow curve for blobs');\n",
    "plt.xlabel('Number of clusters $k$', fontsize = 14);\n",
    "plt.ylabel('$k$-means cost function', fontsize = 14);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the elbow curve is predicting the correct number of blobs -- the biggest change in slope is at 4 clusters. Notice that there's a pretty big change in slope at 3 as well. That's because 2 of the clusters in our generated distribution are closer together than the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDd1sz9noEwY"
   },
   "source": [
    "### First slightly tricky example: overlapping blobs of different size/density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 196,
     "status": "ok",
     "timestamp": 1685198257920,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "2IJsEjSloEwY"
   },
   "outputs": [],
   "source": [
    "X1b, y1b = make_blobs(n_samples=200, centers=[(1.25,1)],\n",
    "                       cluster_std=0.2, random_state=1)\n",
    "\n",
    "X2b, y2b = make_blobs(n_samples=400, centers=[(0,1)],\n",
    "                       cluster_std=0.5, random_state=2)\n",
    "\n",
    "X3b, y3b = make_blobs(n_samples=200, centers=[(-1.25,1)],\n",
    "                       cluster_std=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "executionInfo": {
     "elapsed": 805,
     "status": "ok",
     "timestamp": 1685198259993,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "nIMCuKD0oEwY",
    "outputId": "987a1c09-c994-40d8-cab1-4ef5cea5791b"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.scatter(X1b[:, 0], X1b[:, 1], s =10, c ='gray') # plot original points\n",
    "\n",
    "plt.scatter(X2b[:, 0], X2b[:, 1], s =10, c ='violet') # plot original points\n",
    "\n",
    "plt.scatter(X3b[:, 0], X3b[:, 1], s =10, c ='teal') # plot original points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyeYiYTHoEwZ"
   },
   "source": [
    "Right now, this is labeled data! Let's pretend this is an unsupervised learning problem by combining clusters in single data set, but we'll make a set of the original labels for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 155,
     "status": "ok",
     "timestamp": 1685198261404,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "tEqi8CgfoEwa"
   },
   "outputs": [],
   "source": [
    "Xb = np.vstack([X1b,X2b,X3b])\n",
    "yb = np.concatenate([np.zeros(len(y1b)),np.zeros(len(y2b))+1,np.zeros(len(y3b))+2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuSg9ARfoEwb"
   },
   "source": [
    "Fixing the random seed generates consistent predictions (the predictions of the cluster assignments are usually consistent, but which cluster is predicted as 0, which as 1, which as 2 would otherwise vary.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 144,
     "status": "ok",
     "timestamp": 1685198265207,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "Ys0XZazWoEwb"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, n_init = 10, random_state=30) #predicts 0,1,2\n",
    "kmeans.fit(Xb)\n",
    "yb_kmeans = kmeans.predict(Xb)\n",
    "centersb = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "USXPBEr_oEwb"
   },
   "outputs": [],
   "source": [
    "yb_kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TQ2MmW3oEwc"
   },
   "source": [
    "Now we can compare the cluster assignments from the k means algorithm to the original ones. This isn't something we can normally do with unsupervised learning problems! Notice that k-means has commuted the category labels (colors), but that's ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "executionInfo": {
     "elapsed": 2028,
     "status": "ok",
     "timestamp": 1685198273233,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "vPJim59zoEwc",
    "outputId": "757bbdea-85da-4640-b56b-32cea5bf431c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "model = KMeans(n_clusters=3, n_init = 10, random_state=30) #predicts 0,1,2\n",
    "model.fit(Xb)\n",
    "plot_decision_regions(Xb, yb.astype(int), clf = model, legend=0, markers = '...', colors = 'teal,violet,lightgray')\n",
    "plt.scatter(centersb[:, 0], centersb[:, 1], c='black', s=100, alpha=0.5);\n",
    "\n",
    "plt.xlim(-2.5,2.5)\n",
    "plt.ylim(-0.5,2.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "- Has k-means performed well on this data? Do you expext improved performance from using either DBSCAN or GMM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxj7Qu-PoEwd"
   },
   "source": [
    "### Now we move on to a different distribution (smiley face)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0icw678oEwd"
   },
   "source": [
    "Generate points for the smiley face distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1685198277841,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "aBCKCBJRoEwe"
   },
   "outputs": [],
   "source": [
    "from math import pi, cos, sin\n",
    "from random import random\n",
    "\n",
    "def point(h, k, r):\n",
    "    theta = random() * 2 * pi\n",
    "    return h + cos(theta) * r, k + sin(theta) * r + 0.2*random()\n",
    "\n",
    "xy = [point(1,2,1) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1685198278908,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "MKa8kkV8oEwe"
   },
   "outputs": [],
   "source": [
    "X1, y1 = make_blobs(n_samples=10, centers=[(0.5,2.5)],\n",
    "                       cluster_std=0.05, random_state=1)\n",
    "\n",
    "X2, y2 = make_blobs(n_samples=10, centers=[(1.5,2.5)],\n",
    "                       cluster_std=0.05, random_state=2)\n",
    "\n",
    "X3, y3 = make_blobs(n_samples=10, centers=[(1,1.7)],\n",
    "                       cluster_std=0.05, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1685198279967,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "OY3KgbXJoEwe"
   },
   "outputs": [],
   "source": [
    "X3_stretch = np.array([X3[:,0]*3, X3[:,1]]) #make the mouth :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 708,
     "status": "ok",
     "timestamp": 1685198281585,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "A1xfAyoZoEwg",
    "outputId": "740a221f-157b-45d5-9079-46da82f0f093"
   },
   "outputs": [],
   "source": [
    "plt.axes().set_aspect('equal', 'datalim')\n",
    "plt.scatter(*zip(*xy))\n",
    "plt.scatter(X1[:,0],X1[:,1])\n",
    "plt.scatter(X2[:,0],X2[:,1])\n",
    "plt.scatter(X3_stretch.T[:,0]-1.9,X3_stretch.T[:,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XE4PHbZsoEwh"
   },
   "source": [
    "Finally, put together the four sets of points in the figure to create the data set as a single array:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1685198283884,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "pk9uulbZoEwh"
   },
   "outputs": [],
   "source": [
    "X = np.vstack([xy,X1,X2,np.array([X3_stretch.T[:,0]-1.9,X3_stretch.T[:,1]]).T])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2w6wxFcoEwh"
   },
   "source": [
    "### Let's see how k-means clusters these points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 432,
     "status": "ok",
     "timestamp": 1685198285952,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "BpUhI0NhoEwh",
    "outputId": "0254482e-72fd-4664-dc02-c79f97bd96aa"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, n_init = 10, random_state=32) #We can also change the number of clusters\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=10, cmap='viridis')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=100, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqcFbAYfoEwh"
   },
   "source": [
    "Create vector with true labels (color the points):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1685198288882,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "G8L9N7RBoEwh"
   },
   "outputs": [],
   "source": [
    "y = np.concatenate([np.zeros(len(xy)), np.zeros(len(y1))+1,np.zeros(len(y2))+2,np.zeros(len(y3))+3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1POTsxC8oEwi"
   },
   "source": [
    "Finally, compare the clustering assignments of k means (background color) to the true labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "executionInfo": {
     "elapsed": 779,
     "status": "ok",
     "timestamp": 1685198291681,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "Tqsh4dy-oEwi",
    "outputId": "5dab6b70-b492-4bdd-a97b-f9d7088eb469"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plot_decision_regions(X, y.astype(int), clf = kmeans, legend=0, markers = '.', colors = 'lightgray,teal,yellow,violet')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=100, alpha=0.5);\n",
    "plt.scatter(*zip(*xy), s = 30, c = 'lightgray', edgecolors='k')\n",
    "plt.scatter(X1[:,0],X1[:,1], s = 30, c = 'teal',edgecolors='k')\n",
    "plt.scatter(X2[:,0],X2[:,1], s = 30, c = 'yellow', edgecolors='k')\n",
    "plt.scatter(X3_stretch.T[:,0]-1.9,X3_stretch.T[:,1],s = 30, c = 'violet', edgecolors='k')\n",
    "plt.xlim(-0.5,2.5);\n",
    "plt.ylim(0.5,3.5);\n",
    "#plt.savefig('ClustersBad2.pdf', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03Kct00IoEwi"
   },
   "source": [
    "This isn't very good! Let's see if optimizing the number of clusters using the elbow curve helps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 162,
     "status": "ok",
     "timestamp": 1685198294846,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "xVRKMwraoEwi"
   },
   "outputs": [],
   "source": [
    "inertias = []\n",
    "for k in range(2, 10):\n",
    "    kmeans = KMeans(n_clusters=k, n_init = 10)\n",
    "    kmeans.fit(X)\n",
    "    inertias.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569
    },
    "executionInfo": {
     "elapsed": 499,
     "status": "ok",
     "timestamp": 1685198295523,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "MeQoI4sPoEwi",
    "outputId": "fa6856d8-350f-401f-e70e-7396867e7f0d"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(2, 10), inertias)\n",
    "plt.title('Elbow curve for smiley face');\n",
    "plt.xlabel('Number of clusters $k$', fontsize = 14);\n",
    "plt.ylabel('$k$-means cost function', fontsize = 14);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4tb4VWrpZS6"
   },
   "source": [
    "Again, n = 4 is the preferred choice: this is correct but mostly by chance, as the distribution is not one that k-means can handle correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02IaWLVxoEwj"
   },
   "source": [
    "### Silhouette score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptmxs7TpoEwj"
   },
   "source": [
    "The silhouette score is another metric used to evaluate the success of a clustering method and pick a number of clusters. Higher values correspond to better-defined clustering schemes.\n",
    "\n",
    "Just remember, silhouette scores rely on comparing intra-cluster vs. inter-cluster distances, so they won't give nice-looking results for \"closely spaced spaghetti\" or \"nested arc\"-like clusters! Alternatives that are better for those types of distributions do exist: you can check out \"Density-Based Clustering Validation (DBCV)\" as one example, though it's not built in to sk-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 472,
     "status": "ok",
     "timestamp": 1685198389627,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "WXiANMRQoEwj"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 7997,
     "status": "ok",
     "timestamp": 1685198402119,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "_XkdYS8MoEwk",
    "outputId": "b709be20-e5e3-4371-d818-4dfa0425e02d"
   },
   "outputs": [],
   "source": [
    "#Smiley face\n",
    "\n",
    "n_clusters = [2,3,4,6]\n",
    "\n",
    "for n in n_clusters:\n",
    "    \n",
    "    model = KMeans(n_clusters = n, n_init = 10, random_state=10)\n",
    "\n",
    "    model.fit(X)\n",
    "\n",
    "    y_kmeans = model.predict(X)\n",
    "\n",
    "    silhouette_scores = silhouette_samples(X, y_kmeans)\n",
    "\n",
    "    xlower = 10\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    ax = axs[1]\n",
    "    colors = plt.cm.Accent(y_kmeans.astype(float)/n)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=colors, s=40, cmap='flare', edgecolor='k');\n",
    "    ax.tick_params(axis='both', which='both', labelsize=20);\n",
    "\n",
    "    ax = axs[0]\n",
    "\n",
    "    for i in np.unique(y_kmeans):\n",
    "        ind = y_kmeans==i\n",
    "        silh = np.sort(silhouette_scores[ind])\n",
    "        size_cluster_i = silh.shape[0]\n",
    "        xupper = xlower + size_cluster_i\n",
    "        color = plt.cm.Accent(float(i)/model.n_clusters)\n",
    "        ax.fill_between(np.arange(xlower, xupper), 0, silh, facecolor=color, edgecolor=color, alpha=0.7)\n",
    "        ax.axhline(y=0, c='k', lw=2)\n",
    "        ax.text(0.05, 0.95, '%0.0f clusters'%n, transform=ax.transAxes, fontsize=20)\n",
    "        ax.text(0.45, 0.95, 'Mean S. score: %0.2f'%np.mean(silhouette_scores), transform=ax.transAxes, fontsize=20)\n",
    "        xlower = xupper + 10\n",
    "        ax.set_ylabel('Silhouette score', fontsize=16)\n",
    "        ax.set_ylim(-0.2,0.8)\n",
    "        \n",
    "    ax.axhline(y=np.mean(silhouette_scores), color=\"red\", linestyle=\"--\")\n",
    "    ax.tick_params(axis='both', which='both', labelsize=20);\n",
    "    ax.set_xticks([]);\n",
    "#    figname = 'SilhouetteSmiley'+str(n)+'.pdf'\n",
    "#    plt.savefig(figname, dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnIjbnzCoEwk"
   },
   "source": [
    "In the example above, the silhouette score points to 4 clusters, but keep in mind that any of these methods (elbow method, silhouette score) only makes sense if the hypothesis that clustered can be correctly identified by k-means (in other words, are convex, globular, and similar in shape and density). This is not the case here! That's also why we see a higher average silhouette score the more clusters we add."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7pttxJuoEwk"
   },
   "source": [
    "### We can now take a quick look at two density-based clustering methods, DBSCAN and OPTICS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 145,
     "status": "ok",
     "timestamp": 1685198407073,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "54zbilKIoEwk"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "#Code adapted from: https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=0.25, min_samples=2).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "executionInfo": {
     "elapsed": 975,
     "status": "ok",
     "timestamp": 1685198410433,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "OXUOwEVcoEwl",
    "outputId": "c2c54692-35cf-4bae-8dad-786f1e4d80ed"
   },
   "outputs": [],
   "source": [
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUxcUP0woEwl"
   },
   "source": [
    "We can see how the clustering changes as we vary the distance parameter, eps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2555,
     "status": "ok",
     "timestamp": 1685198415046,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "POW9hDiWoEwl",
    "outputId": "b8e89162-68cb-4bfe-d0e6-3b4ae87e3eba",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Compute DBSCAN\n",
    "\n",
    "for i,eps in enumerate([0.2, 0.25, 0.3, 0.35]):\n",
    "    \n",
    "    plt.figure(figsize = (6,6))\n",
    "    \n",
    "    db = DBSCAN(eps=eps, min_samples=2).fit(X)\n",
    "\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "\n",
    "    print('Estimated number of clusters: %d' % n_clusters_)\n",
    "    print('Estimated number of noise points: %d' % n_noise_)\n",
    "\n",
    "# #############################################################################\n",
    "\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "    unique_labels = set(labels)\n",
    "    colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "        # Black used for noise.\n",
    "            col = [0, 0, 0, 1]\n",
    "\n",
    "        class_member_mask = (labels == k)\n",
    "\n",
    "        xy = X[class_member_mask & core_samples_mask]\n",
    "        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=10)\n",
    "\n",
    "        xy = X[class_member_mask & ~core_samples_mask]\n",
    "        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=6)\n",
    "\n",
    "    plt.title('$\\\\epsilon$ = %0.2f; estimated number of clusters: %d' % (eps, n_clusters_))\n",
    "    \n",
    "    plt.savefig('DBSCAN_'+str(i)+'.pdf', dpi = 300)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- How does DBSCAN perform for this distribution? \n",
    "- What does the epsilon parameter control?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdOUgURuoEwn"
   },
   "source": [
    "My final note here is that building and evaluating clustering schemes is tricky. For our clustering to make sense, we need to have a good knowledge of the structure of our data, but unfortunately, this is often what we'd like to learn by building the clustering in first place. I don't have a lot of wisdom to share; just this word of caution. On Thursday we'll discuss dimensionality reduction methods, which you can use (with caution!) to visualize the structure of your data, which can help you choose a clustering method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTOQmqlLoEwn"
   },
   "source": [
    "### Finally, we will take a look at how to use a probabilistic model: Gaussian Mixture Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfxtMrZ_oEwn"
   },
   "source": [
    "Again from sklearn docs: A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. One can think of mixture models as generalizing k-means clustering to incorporate information about the covariance structure of the data as well as the centers of the latent Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 154,
     "status": "ok",
     "timestamp": 1685198426452,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "qJpePFnnoEwn"
   },
   "outputs": [],
   "source": [
    "from sklearn import mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 133,
     "status": "ok",
     "timestamp": 1685198427211,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "ikpyvs03oEwn"
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1685198428452,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "qjce3A61oEwo"
   },
   "outputs": [],
   "source": [
    "#Function from https://jakevdp.github.io/PythonDataScienceHandbook/06.00-figure-code.html#Covariance-Type\n",
    "\n",
    "def draw_ellipse(gmm, ax, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    for n in range(gmm.n_components):\n",
    "        if gmm.covariance_type == 'full':\n",
    "            covariances = gmm.covariances_[n]\n",
    "        elif gmm.covariance_type == 'tied':\n",
    "            covariances = gmm.covariances_\n",
    "        elif gmm.covariance_type == 'diag':\n",
    "            covariances = np.diag(gmm.covariances_[n])\n",
    "        elif gmm.covariance_type == 'spherical':\n",
    "            covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n]\n",
    "        v, w = np.linalg.eigh(covariances)\n",
    "        u = w[0] / np.linalg.norm(w[0])\n",
    "        angle = np.arctan2(u[1], u[0])\n",
    "        angle = 180 * angle / np.pi  # convert to degrees\n",
    "        v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "        \n",
    "        # Draw the Ellipse\n",
    "        for nsig in range(1, 4): #1, 2, and 3 sigma\n",
    "            ell = Ellipse(gmm.means_[n], nsig *v[0], nsig *v[1], angle = angle, **kwargs)\n",
    "            ax.add_patch(ell)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a demonstration of what the different covariance options look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "executionInfo": {
     "elapsed": 717,
     "status": "ok",
     "timestamp": 1685198430370,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "vw1f1qnLoEwo",
    "outputId": "8dfe7c3a-3291-48c2-a371-733065c59235"
   },
   "outputs": [],
   "source": [
    "#Figure also adapted from https://jakevdp.github.io/PythonDataScienceHandbook/06.00-figure-code.html#Covariance-Type\n",
    " \n",
    "fig, ax = plt.subplots(1, 3, figsize=(14, 4), sharey=True)\n",
    "fig.subplots_adjust(wspace=0.05)\n",
    "\n",
    "rng = np.random.RandomState(20)\n",
    "Xe = np.dot(rng.randn(500, 2), rng.randn(2, 2))\n",
    "\n",
    "for i, cov_type in enumerate(['full','diag', 'spherical']):\n",
    "    model = mixture.GaussianMixture(n_components=1, covariance_type=cov_type).fit(Xe)\n",
    "    ax[i].axis('equal')\n",
    "    ax[i].scatter(Xe[:, 0], Xe[:, 1], edgecolor='k', alpha=0.5)\n",
    "    ax[i].set_title('covariance_type=\"{0}\"'.format(cov_type), size=14, family='monospace')\n",
    "    draw_ellipse(gmm=model, ax=ax[i], alpha=0.1, edgecolor='k', facecolor='#808080')\n",
    "    ax[i].xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax[i].yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax[i].set_xlim(-5, 5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "- In the plots above, what is being shown by the contour lines?\n",
    "- What is the difference between the \"full,\" \"diag,\" and \"spherical\" covariance options, in terms of the symmetry and \"stretching\" of the Gaussians used for clustering?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vG4uP_NQoEwo"
   },
   "source": [
    "### Modeling with a Gaussian Mixture Model predicts probabilities.\n",
    "\n",
    "We'll start with the 3 blob example from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1685198435349,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "CWuoZMg-oEwo"
   },
   "outputs": [],
   "source": [
    "model = mixture.GaussianMixture(n_components=3, covariance_type='full',random_state=30) \n",
    "\n",
    "model.fit(Xb)\n",
    "\n",
    "y_GMM = model.predict(Xb)\n",
    "\n",
    "probs = model.predict_proba(Xb)\n",
    "\n",
    "size = 50 * probs.max(axis=1)**4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 148,
     "status": "ok",
     "timestamp": 1685198436563,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "awY-iVHHoEwo",
    "outputId": "30a2c074-bc3c-4270-984e-91c04f130526",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_GMM # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fafMdbpOoEwo"
   },
   "source": [
    "### GMM decision function for a \"full\" covariance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 1067,
     "status": "ok",
     "timestamp": 1685198441582,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "iWhVny-koEwo",
    "outputId": "0222c47b-a9b1-45f6-d599-5107186abc89"
   },
   "outputs": [],
   "source": [
    "model = mixture.GaussianMixture(n_components=3, covariance_type='full',random_state=41)\n",
    "\n",
    "model.fit(Xb)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "plt.axis('equal')\n",
    "\n",
    "plot_decision_regions(Xb, yb.astype(int), \n",
    "        clf=model, legend=0, markers = '...', colors = 'lightgray,violet,teal')\n",
    "\n",
    "ax.set_title('covariance_type=\"{0}\"'.format('full'), size=14, family='monospace')\n",
    "\n",
    "ax.set_xlim(-2.5, 2.5)\n",
    "ax.set_ylim(-0.5,2.5)\n",
    "\n",
    "#plt.savefig('GMM_blobs_full.pdf', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8L6vGJzoEwp"
   },
   "source": [
    "As before, the labels are permuted, but that's fine. \n",
    "\n",
    "### Now, for comparison, with a spherical covariance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 1210,
     "status": "ok",
     "timestamp": 1685198451510,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "wS7SDiM5oEwp",
    "outputId": "e56621c5-d386-40bf-8dca-8f0a0642e462"
   },
   "outputs": [],
   "source": [
    "model = mixture.GaussianMixture(n_components=3, covariance_type='spherical',random_state=41)\n",
    "\n",
    "model.fit(Xb)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "plt.axis('equal')\n",
    "\n",
    "plot_decision_regions(Xb, yb.astype(int), \n",
    "        clf=model, legend=0, markers = '...', colors = 'lightgray,violet,teal')\n",
    "\n",
    "ax.set_title('covariance_type=\"{0}\"'.format('spherical'), size=14, family='monospace')\n",
    "\n",
    "ax.set_xlim(-2.5, 2.5)\n",
    "ax.set_ylim(-0.5,2.5)\n",
    "\n",
    "#plt.savefig('GMM_blobs_spherical.pdf', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "- Which performs better for this problem: k-means or GMM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YgAbtI3oEwp"
   },
   "source": [
    "### Finally, we can apply GMMs to the smiley face problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_oY59hNoEwp"
   },
   "source": [
    "We start with 4 components.\n",
    "### Question:\n",
    "- Do you expect this to work? Why or why not? Consider the assumption(s) built in to the GMM method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "executionInfo": {
     "elapsed": 1356,
     "status": "ok",
     "timestamp": 1685198456455,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "jlWRHFyooEwp",
    "outputId": "972209be-2045-4b46-842f-45fb0367b1d8"
   },
   "outputs": [],
   "source": [
    "gmm4 = mixture.GaussianMixture(n_components=4, covariance_type='full', random_state=1)\n",
    "\n",
    "gmm4.fit(X)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "xy = [point(1,2,1) for _ in range(100)]\n",
    "\n",
    "plot_decision_regions(X, y.astype(int), \n",
    "        clf=gmm4, legend=0, markers = '.', colors = 'lightgray,yellow,teal,violet')\n",
    "\n",
    "plt.xlim(-0.5,2.5);\n",
    "\n",
    "plt.ylim(0.5,3.5);\n",
    "\n",
    "#plt.savefig('GMMbad.pdf', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNqwC_MboEwp"
   },
   "source": [
    "### We can use the BIC criterion to figure out how many Gaussian mixture components best fit the smiley face in the GMM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ky48Z_OhoEwp"
   },
   "source": [
    "Note that results tend to vary slightly between different runs; in the book we show a run for 10 components, but obtaining 9 or 11 (or even a second minimum at 13) is not uncommon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570
    },
    "executionInfo": {
     "elapsed": 1503,
     "status": "ok",
     "timestamp": 1685198462805,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "1jmUNnYSoEwp",
    "outputId": "d3abe24c-932d-4ef5-cde0-20a841f73106"
   },
   "outputs": [],
   "source": [
    "n_components = np.arange(1, 30)\n",
    "\n",
    "models = [mixture.GaussianMixture(n, covariance_type='full', random_state=10).fit(X)\n",
    "          for n in n_components]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(n_components, [m.bic(X) for m in models], label='BIC')\n",
    "ax.legend(loc='best', fontsize=20)\n",
    "ax.set_xlabel('n_components', fontsize=20);\n",
    "ax.tick_params(axis='both', which='both', labelsize=20);\n",
    "#plt.savefig('GMM_smiley_BIC.pdf', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szXkkFsToEwq"
   },
   "source": [
    "### And re-do the plot with the appropriate number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1685198470665,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "N7k3JomZoEwq"
   },
   "outputs": [],
   "source": [
    "#These two functions (note that \"draw_ellipse\" is not the same as before!) are also from Jake Vanderplas' notebooks.\n",
    "\n",
    "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Convert covariance to principal axes\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0.0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "    \n",
    "    # Draw the Ellipse\n",
    "    for nsig in range(1, 4):\n",
    "        ax.add_patch(Ellipse(position, width = nsig * width, height = nsig * height, angle=angle, **kwargs))\n",
    "\n",
    "def plot_gmm(gmm, X, label=True, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    labels = gmm.fit(X).predict(X)\n",
    "    if label:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='Accent', zorder=2, edgecolor='k')\n",
    "    else:\n",
    "        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2, edgecolor='k')\n",
    "    ax.axis('equal')\n",
    "    \n",
    "    w_factor = 0.2 / gmm.weights_.max()\n",
    "    \n",
    "    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n",
    "        draw_ellipse(pos, covar, facecolor='#808080', edgecolor='k', alpha=w * w_factor)\n",
    "        \n",
    "    ax.tick_params(axis='both', which='both', labelsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "executionInfo": {
     "elapsed": 716,
     "status": "ok",
     "timestamp": 1685198473451,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "kuDj7ccmoEwq",
    "outputId": "d296cc23-971f-475c-dd6a-3fce1123d3c6"
   },
   "outputs": [],
   "source": [
    "gmm10 = mixture.GaussianMixture(n_components=10, covariance_type='full', random_state=0)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax = fig.add_subplot(111, aspect='equal')\n",
    "\n",
    "plot_gmm(gmm10, X, label=False, ax=ax)\n",
    "\n",
    "plt.xlim(-0.5,2.5);\n",
    "\n",
    "plt.ylim(0.5,3.5);\n",
    "\n",
    "plt.text(-0.3,0.7,'Original', fontsize = 18)\n",
    "\n",
    "ax.tick_params(axis='both', which='both', labelsize=20);\n",
    "\n",
    "#plt.savefig('Smiley_GMM_10.pdf', dpi = 300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "- How does the classification approach taken by GMM differ from those taken by k-means and DBSCAN? Does each point correspond to just a single cluster, in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMnNAtjnoEwq"
   },
   "source": [
    "### Finally, we can use our GMM with 10 components as a generative model to generate new samples that follow the smiley face distribution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 719
    },
    "executionInfo": {
     "elapsed": 994,
     "status": "ok",
     "timestamp": 1685198482571,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "eNW80MA7oEwq",
    "outputId": "1f1954fe-dac2-408d-ee9e-4c180d4ed43e"
   },
   "outputs": [],
   "source": [
    "Xnew = gmm10.sample(n_samples=500) #This is how we generate new samples!\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax2 = fig.add_subplot(111, aspect='equal', sharey = ax)\n",
    "\n",
    "ax2.scatter(Xnew[0][:, 0], Xnew[0][:, 1], s = 40, facecolor='r', edgecolor='k', alpha=0.5);\n",
    "\n",
    "ax2.tick_params(axis='both', which='both', labelsize=20);\n",
    "\n",
    "plt.xlim(-0.5,2.5);\n",
    "\n",
    "plt.ylim(0.5,3.5);\n",
    "\n",
    "plt.text(-0.3,0.7,'Generated', fontsize = 18)\n",
    "\n",
    "#plt.savefig('Smiley_GMM_generated.pdf', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, but also not perfect! Since our GMM clustering wasn't able to separate the right eye from the outline of the smiley face into two separate clusters, the generated data will reproduce this error. Something to be careful about with generative AI methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, move on to testing clustering for galaxy classification in the $\\texttt{Studio10\\_GalaxyClustering.ipynb}$ notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgement statement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload both notebooks to Gradescope for this week!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PHYS448",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
