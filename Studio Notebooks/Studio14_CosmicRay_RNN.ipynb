{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daXuA3h2b53x"
   },
   "source": [
    "# Studio 14: Finding Cosmic Ray Signals with RNNs\n",
    "\n",
    "Adapted from https://deeplearningforphysicsresearchbook.github.io/deep-learning-physics/ by Javier Duarte (2023)\n",
    "\n",
    "Further modifications by Julieta Gruszko (2025)\n",
    "\n",
    "Large arrays of radio antennas can be used to measure cosmic rays by recording the electromagnetic radiation generated in the atmosphere.\n",
    "These radio signals are strongly contaminated by galactic noise as well as signals from human origin. Since these signals appear to be similar to the background, the discovery of cosmic-ray events can be challenging.\n",
    "\n",
    "The data file for this studio is a bit too large to host on Github. It's available on Canvas instead, go ahead and download it from there.\n",
    "\n",
    "## Identification of signals\n",
    "In this exercise, we design an RNN to classify if the recorded radio signals contain a cosmic-ray event or only noise.\n",
    "\n",
    "The signal-to-noise ratio (SNR) of a measured trace $S(t)$ is defined as follows:\n",
    "\n",
    "$SNR=\\frac{|S(t)_{max}|}{RMS[S(t)]}$\n",
    "\n",
    "where $|S(t)_{max}|$ denotes the maximum amplitude of the (true) signal.\n",
    "\n",
    "Typical cosmic-ray observatories enable a precise reconstruction at an SNR of roughly 3.\n",
    "\n",
    "We choose a challenging setup in this task and try to identify cosmic-ray events in signal traces with an SNR of 2.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training RNNs can be computationally demanding, thus, we recommend to use a GPU for this task if possible.\n",
    "### Depending on your computer, you should run just one of the two options below to switch to GPU, if available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Macs:\n",
    "If you have an Apple Silicon chip in your laptop (e.g. M1 or above), you can use the integrated GPU by running the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!xcode-select --install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple GPU with Metal backend.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Others:\n",
    "If you have a non-Mac computer, you can check for an available GPU with by running this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available for PyTorch.\")\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU device name: {torch.cuda.get_device_name(0)}\") # Get name of the first GPU\n",
    "else:\n",
    "    print(\"GPU is NOT available for PyTorch. Using CPU.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras import Input\n",
    "\n",
    "from keras.models import Sequential #the model is built adding layers one after the other\n",
    "\n",
    "from keras.layers import Dense #fully connected layers: every output talks to every input\n",
    "\n",
    "from keras.layers import Activation\n",
    "\n",
    "from keras.layers import SimpleRNN, LSTM, Bidirectional, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7A1BSCHb539"
   },
   "source": [
    "### Load and prepare dataset\n",
    "In this task, we use a simulation of cosmic-ray-induced air showers that are measured by radion antennas.  \n",
    "For more information, see https://arxiv.org/abs/1901.04079.  \n",
    "The task is to design an RNN which is able to identify if the measured signal traces (shortened to 500 time steps) contains a signal or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.load(\"../Data/cosmic-radio-showers_with_signals.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file contains measured traces, labels, and for cosmic-ray events, the true signal. For non cosmic-ray events, the signal object is set to 0. The traces and signals are each 500 samples long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot example signal traces\n",
    "Left: signal trace containing a cosmic-ray event. The underlying cosmic-ray signal is shown in red, the backgrounds + signal is shown in blue.\n",
    "Right: background noise, with 0 underlying signal trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fs = 180e6  # Sampling frequency of antenna setup 180 MHz\n",
    "t = np.arange(500) / fs * 1e6\n",
    "idx_sig = 54 #pick the n'th signal trace in the file\n",
    "idx_noise = 17 #pick the n'th noise trace in the file\n",
    "\n",
    "plt.figure(1, (12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(t, np.real(f[\"traces\"][np.where(f[\"labels\"] == 1)][idx_sig]), linewidth = 1, color=\"b\", label=\"Measured trace\")\n",
    "plt.plot(t, np.real(f[\"signals\"][np.where(f[\"labels\"] == 1)][idx_sig]), linewidth = 1, color=\"r\", label=\"CR signal\")\n",
    "plt.ylabel('Amplitude / mV')\n",
    "plt.xlabel('Time / $\\mu$ s')\n",
    "plt.legend()\n",
    "plt.title(\"Cosmic-ray event\")\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plt.plot(t, np.real(f[\"traces\"][np.where(f[\"labels\"] == 0)][idx_noise]), linewidth = 1, color=\"b\", label=\"Measured trace\")\n",
    "plt.plot(t, np.real(f[\"signals\"][np.where(f[\"labels\"] == 0)][idx_noise]), linewidth = 1, color=\"r\", label=\"CR signal\")\n",
    "\n",
    "plt.ylabel('Amplitude / mV')\n",
    "plt.xlabel('Time / $\\mu$ s')\n",
    "plt.legend()\n",
    "plt.title(\"Noise event\")\n",
    "\n",
    "plt.grid(True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(f['traces'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(f['labels'][f['labels']==1])) # cosmic ray events\n",
    "print(np.shape(f['labels'][f['labels']==0])) # noise events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- How many instances are in our learning set?\n",
    "- Is the data set balanced or unbalanced?\n",
    "- In this notebook, we'll try to identify whether a signal is present in each trace. When type of RNN structure do we need to use? E.g. sequence-to-sequence, vector-to-sequence, or sequence-to-vector? \n",
    "- What type of task are we performing (e.g. classification, regression, generation)? What type of activation function should our final layer use? What type of loss should we optimize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did last week, we will use sk-learn's $\\texttt{train\\_test\\_split}$ to split the data in to random selections with appropriate fractions. This time, instead of making a validation set ourselves, we'll use Keras's built-in validation split option. We'll just split off test data for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the samples into training and test data sets:\n",
    "train_frac = 0.8\n",
    "test_frac = 0.2\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    f[\"traces\"], f['labels'], test_size=test_frac, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "Let's scale the data so that the train data traces have standard deviation of 1. We won't bother shifting the mean, since the data traces are already centered around 0. We could use sk-learn's Standard Scaler object to do this, but it's also easy enough to do it by hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRPX-EYOb54L"
   },
   "outputs": [],
   "source": [
    "sigma = x_train.std()\n",
    "x_train /= sigma\n",
    "x_test /= sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOanYtv_b54G"
   },
   "source": [
    "### Define RNN model\n",
    "In the following, design a cosmic-ray model to identify cosmic-ray events using an RNN-based classifier.\n",
    "\n",
    "We'll start with a Simple RNN, then try a bi-directional RNN, then finally a one-direction LSTM. \n",
    "\n",
    "By default, the output of an RNN layer contains a single vector per sample. This vector is the RNN cell output corresponding to the last timestep, containing information about the entire input sequence. The shape of this output is $\\texttt{(batch\\_size, units)}$ where $\\texttt{units}$ corresponds to the units argument passed to the layer's constructor.\n",
    "\n",
    "A RNN layer can also return the entire sequence of outputs for each sample (one vector per timestep per sample), if you set $\\texttt{return\\_sequences=True}$. The shape of this output is $\\texttt{(batch\\_size, timesteps, units)}$.\n",
    "\n",
    "$\\texttt{return\\_sequences=True}$ is used when you want to return a sequence (e.g. one-to-many and many-to-many RNNs). When you're stacking RNN layers, as we do below, each RNN layer is expecting a sequence as an input, so you'll want to keep it set to $\\texttt{True}$. In the final RNN layer for a many-to-one RNN, we can choose: either we can either use an RNN layer as the last step, setting it to $\\texttt{False}$ to return a single value, or we can output the sequence from our last RNN layer, sending it to a fully connected layer with a single output for the last step.\n",
    "\n",
    "\n",
    "## RNN 1: Simple RNN\n",
    "\n",
    "This model is probably a bit too shallow to perform really well, but we'll keep it simple to reduce the training time to something that can be done in class. The commented-out layer can be reintroduced if you want to try something deeper later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = Sequential()\n",
    "model_rnn.add(keras.Input(shape=(500, 1)))\n",
    "model_rnn.add(SimpleRNN(32, return_sequences=True, recurrent_initializer=\"glorot_uniform\"))\n",
    "#model_rnn.add(SimpleRNN(64, return_sequences=True, recurrent_initializer=\"glorot_uniform\"))\n",
    "model_rnn.add(SimpleRNN(10, return_sequences=True, recurrent_initializer=\"glorot_uniform\"))\n",
    "model_rnn.add(Flatten())\n",
    "model_rnn.add(Dropout(0.3))\n",
    "model_rnn.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- How many hidden layers does this model have?\n",
    "- How many free parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model\n",
    "\n",
    "We'll use AdamW, our old standby optimizer. This time we'll set the $\\texttt{weight\\_decay}$ option to a smaller value than the default; this parameter controls how strong the regularization is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=keras.optimizers.AdamW(1e-3, weight_decay=0.00008),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model on GPU\n",
    "\n",
    "To actually run the training on GPU, we'll need to explicitly move the data over to the GPU. Once the data is on GPU, Keras automatically handles sending the model to GPU for training. To move the data over, we first need to switch it from a numpy array to a torch tensor. \n",
    "\n",
    "If you ran the right cells above, $\\texttt{device}$ should be set correctly for your computer (CPU if you don't have a GPU, or the appropriate GPU options for Macs or non-Macs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.from_numpy(x_train)\n",
    "x_train = x_train.to(device)\n",
    "\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_train = y_train.to(device)\n",
    "\n",
    "x_test = torch.from_numpy(x_test)\n",
    "x_test = x_test.to(device)\n",
    "\n",
    "y_test = torch.from_numpy(y_test)\n",
    "y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the fit, we're going to use a couple of new options that should help the optimization process avoid some of the pitfalls we saw last week:\n",
    "- Reduce the learning rate when the loss stops improving, so your optimizer takes smaller steps when it's getting close to the minimum.\n",
    "- Apply early stopping, so training stops if the loss doesn't improve after some number of generations.\n",
    "\n",
    "Even using a GPU, training RNN's is very slow: they need to process samples one at a time. On my laptop (using an M2 GPU) this model took about 26 seconds per epoch to train; about 13 minutes in total. \n",
    "\n",
    "30 epochs is probably a bit short, but it should be close enough while keeping to our time constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rnn = model_rnn.fit(x_train[...,np.newaxis], y_train,\n",
    "                    batch_size=1024,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks = [keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, min_delta=.0005, verbose=1, min_lr=1e-5),\n",
    "                                 keras.callbacks.EarlyStopping(patience=10, min_delta=.0001, verbose=1)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- Describe the conditions that will lead the learning rate to be reduced, given our settings above.\n",
    "- Did the automated loss rate reduction we applied kick in during training? If yes, how many times was a reduction in loss rate applied?\n",
    "- Describe the conditions that will lead to early stopping, given our settings above.\n",
    "- Did early stopping kick in during training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, let's see the results on the test data. This will also take a little time, since evalaution in RNN's also happens sequentially. On my laptop, it took 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn.evaluate(x_test[...,np.newaxis], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, (12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(results_rnn.history['loss'])\n",
    "plt.plot(results_rnn.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(results_rnn.history['accuracy'])\n",
    "plt.plot(results_rnn.history['val_accuracy'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "- Did the model converge?\n",
    "- Does the model have high bias, high variance, both, or neither?\n",
    "- How well does our model perform on the test data?\n",
    "- What would you suggest we try to improve the model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# These two networks take longer to train. I suggest each group member train one of them in studio, and then you can compare your results. \n",
    "If you have extra time, you can go back and train the one you skipped.\n",
    "\n",
    "\n",
    "## RNN 2: Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_birnn = Sequential()\n",
    "model_birnn.add(keras.Input(shape=(500, 1)))\n",
    "model_birnn.add(Bidirectional(SimpleRNN(32, return_sequences=True, recurrent_initializer=\"glorot_uniform\")))\n",
    "#model_rnn.add(Bidirectional(SimpleRNN(64, return_sequences=True, recurrent_initializer=\"glorot_uniform\")))\n",
    "model_birnn.add(Bidirectional(SimpleRNN(10, return_sequences=True, recurrent_initializer=\"glorot_uniform\")))\n",
    "model_birnn.add(Flatten())\n",
    "model_birnn.add(Dropout(0.3))\n",
    "model_birnn.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model_birnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- How many hidden layers does this model have?\n",
    "- How many free parameters?\n",
    "- Why does this model have more parameters than our first RNN model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and fit the model\n",
    "\n",
    "With more parameters, this model takes longer to train! We're training 4 RNNs here, not 2: for each RNN layer, we train one that moves forward through the sequence and one that moves backward through the sequence.\n",
    "\n",
    "For me, this network took ~1m per epoch to train, so about 30 mins in total.\n",
    "\n",
    "Feel free to have an project group check-in discussion while your networks train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_birnn.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=keras.optimizers.AdamW(1e-3, weight_decay=0.00008),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_birnn = model_birnn.fit(x_train[...,np.newaxis], y_train,\n",
    "                    batch_size=1024,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks = [keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, min_delta=.0005, verbose=1, min_lr=1e-5),\n",
    "                                 keras.callbacks.EarlyStopping(patience=10, min_delta=.0001, verbose=1)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- Did the automated loss rate reduction we applied kick in during training? If yes, how many times was a reduction in loss rate applied?\n",
    "- Did early stopping kick in during training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, let's see the results on the test data. This will also take a little time, since evalaution in RNN's also happens sequentially. On my laptop, it took 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_birnn.evaluate(x_test[...,np.newaxis], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, (12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(results_birnn.history['loss'])\n",
    "plt.plot(results_birnn.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(results_birnn.history['accuracy'])\n",
    "plt.plot(results_birnn.history['val_accuracy'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "- Did the model converge?\n",
    "- Does the model have high bias, high variance, both, or neither?\n",
    "- How well does our model perform on the test data?\n",
    "- What would you suggest we try to improve the model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## RNN 3: LSTM\n",
    "\n",
    "This time, we'll try a relatively shallow uni-directional LSTM for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(keras.Input(shape=(500, 1)))\n",
    "model_lstm.add(LSTM(32, return_sequences=True, recurrent_initializer=\"glorot_uniform\"))\n",
    "#model_lstm.add(LSTM(64, return_sequences=True, recurrent_initializer=\"glorot_uniform\"))\n",
    "model_lstm.add(LSTM(10, return_sequences=True, recurrent_initializer=\"glorot_uniform\"))\n",
    "model_lstm.add(Flatten())\n",
    "model_lstm.add(Dropout(0.3))\n",
    "model_lstm.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- How many hidden layers does this model have?\n",
    "- How many free parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bw8tkc1Xb54I"
   },
   "source": [
    "#### LSTM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=keras.optimizers.AdamW(1e-3, weight_decay=0.00008),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More parameters mean longer training times: this one took ~1m per epoch on my laptop, about 28 minutes in total.\n",
    "\n",
    "Feel free to have an project group check-in discussion while your networks train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lstm = model_lstm.fit(x_train[...,np.newaxis], y_train,\n",
    "                    batch_size=1024,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks = [keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, min_delta=.0005, verbose=1, min_lr=1e-5),\n",
    "                                 keras.callbacks.EarlyStopping(patience=10, min_delta=.0001, verbose=1)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- Did the automated loss rate reduction we applied kick in during training? If yes, how many times was a reduction in loss rate applied?\n",
    "- Did early stopping kick in during training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.evaluate(x_test[...,np.newaxis], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, (12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(results_lstm.history['loss'])\n",
    "plt.plot(results_lstm.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(results_lstm.history['accuracy'])\n",
    "plt.plot(results_lstm.history['val_accuracy'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "- Did the model converge?\n",
    "- Does the model have high bias, high variance, both, or neither?\n",
    "- How well does our model perform on the test data?\n",
    "- What would you suggest we try to improve the model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion Questions:\n",
    "- Based on the 3 networks, does adding more information about long-term correlations in the signal improve performance? Which networks should you compare to evaluate this? Connect your discussion to the cosmic ray signal shape you observed during the initial data exploration.\n",
    "- Based on the 3 networks, does adding more information about backwards-running correlations in the signal improve performance? Which networks should you compare to evaluate this? Connect your discussion to the cosmic ray signal shape you observed during the initial data exploration.\n",
    "- Can we conclude that for this task, LSTMs and bi-directionality won't ever help? Or are there other changes we should make before re-evaluating those questions? If so, describe the changes you'd suggest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgement Statement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, you're done with studio for today! Upload your notebook to Gradescope, making sure to merge the written responses for all 3 RNNs into a single notebook, even if all 3 weren't trained in that notebook. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Exercise_9_2_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PHYS448_DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
