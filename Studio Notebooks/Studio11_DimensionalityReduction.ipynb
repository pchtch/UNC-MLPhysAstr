{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nxg0SwXqGe2"
   },
   "source": [
    "This is a simple notebook to do PCA on SDSS spectra and galaxy images.\n",
    "\n",
    "It accompanies Chapter 7 of the book (2 of 4).\n",
    "\n",
    "Copyright: Viviana Acquaviva (2023); see also other code credits below.\n",
    "\n",
    "Modifications by Julieta Gruszko (2025)\n",
    "\n",
    "License: [BSD-3-clause](https://opensource.org/license/bsd-3-clause/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbQHEHnfqGe3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing, decomposition\n",
    "import skimage\n",
    "from skimage.transform import resize, rescale\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDbaO3X4qGe4"
   },
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__rSM0vyqGe4"
   },
   "source": [
    "Principal Component Analysis (PCA) and similar algorithms are used for dimensionality reduction in data-intensive sciences. \n",
    "\n",
    "The main goal of linear PCA is to find the most representative linear combinations of features, so that each element of a data set can be expressed as the superposition (sum) of some salient vectors in feature space (they don't need to be elements of a data set. In the simplest linear PCA, the principal components are the eigenvectors of the covariance matrix of the data set.\n",
    "\n",
    "If the number of components is very low (e.g. 2 or 3, PCA or other Dimensionality Reduction methods allow one to visualize a high-dimensional data set as a 2D or 3D plot. Scikit-learn has methods to compute PCA and several variants. Classic PCA has tough complexity: $\\mathcal{O}[N^3].$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHCYtwSlqGe4"
   },
   "source": [
    "### Let's look at an example with galaxy spectra from \n",
    "\n",
    "https://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/tutorial/astronomy/dimensionality_reduction.html#sdss-spectral-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0Gud_FwqGe5"
   },
   "outputs": [],
   "source": [
    "data = np.load('../Data/spec4000_corrected.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pjbX6CeFqGe5"
   },
   "outputs": [],
   "source": [
    "wavelengths = data['wavelengths']\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "labels = data['labels'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KYUjP1DqGe5",
    "outputId": "dc6c4f36-de91-4af7-810b-51cad14ac708"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZRgB136qGe6",
    "outputId": "6cc47d94-05d9-46f1-f501-c1b10a7bdf6f"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ct9m-r7RqGe6",
    "outputId": "516c7c95-1c4b-4a2f-aef3-bbef002f4bde"
   },
   "outputs": [],
   "source": [
    "labels #we don't really care about these; they are only used in the next cell to show some example spectra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ch5mDTIqGe7"
   },
   "source": [
    "### We can plot some representative examples from each class, just to have a sense of what kind of spectra are in the data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8uAtGnaQqGe7",
    "outputId": "2b79c1b3-d7db-4976-c72e-81981ecf38f7"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "for i_class in (2, 3, 4, 5, 6):\n",
    "    i = np.where(y == i_class)[0][0]\n",
    "    l = plt.plot(wavelengths, X[i] + 20 * i_class)\n",
    "    c = l[0].get_color()\n",
    "    plt.text(6800, 2 + 20 * i_class, labels[i_class], color=c)\n",
    "\n",
    "plt.subplots_adjust(hspace=0)\n",
    "plt.xlabel('wavelength (Angstroms)')\n",
    "plt.ylabel('flux + offset')\n",
    "plt.title('Sample of Spectra');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "- How many instances are in our data set? How many features?\n",
    "- What are the features of the data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOk5NqyWqGe8"
   },
   "source": [
    "We will try to represent our data with a variable amount of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuooeKT-qGe8"
   },
   "outputs": [],
   "source": [
    "#  Perform PCA\n",
    "\n",
    "scaler = preprocessing.StandardScaler() #It's important that data are centered!\n",
    "\n",
    "Xn = scaler.fit_transform(X) #This is a standardization procedure.\n",
    "\n",
    "pca_50 = decomposition.PCA(n_components=50, random_state=0)\n",
    "\n",
    "pca_100 = decomposition.PCA(n_components=100, random_state=0)\n",
    "\n",
    "pca_1000 = decomposition.PCA(n_components=1000, random_state=0)\n",
    "\n",
    "X_proj_50 = pca_50.fit_transform(Xn) \n",
    "\n",
    "X_proj_100 = pca_100.fit_transform(Xn) \n",
    "\n",
    "X_proj_1000 = pca_1000.fit_transform(Xn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- How many instances does $\\texttt{X\\_proj\\_50}$ have? How many features?\n",
    "- How many instances does $\\texttt{X\\_proj\\_100}$ have? How many features?\n",
    "- How many instances does $\\texttt{X\\_proj\\_1000}$ have? How many features?\n",
    "- Is $\\texttt{X\\_proj\\_1000}$ identical to the original data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2okWFQoqGe8",
    "outputId": "6936c003-130b-42ec-efdd-b869972248b8"
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "#\n",
    "#  plot PCA eigenspectra\n",
    "#\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "l = plt.plot(wavelengths, pca_50.mean_ - 0.15)\n",
    "c = l[0].get_color()\n",
    "plt.text(7000, -0.16, \"mean\", color=c) \n",
    "\n",
    "# In linear PCA, the first eigenvector is always the mean, \n",
    "# and the first n components are always the same\n",
    "\n",
    "for i in range(4):\n",
    "    \n",
    "    l = plt.plot(wavelengths, pca_50.components_[i] + 0.15 * i)\n",
    "    \n",
    "    l = plt.plot(wavelengths, pca_100.components_[i] + 0.15 * i, linestyle = '-.')\n",
    "    \n",
    "    c = l[0].get_color()\n",
    "    \n",
    "    plt.text(7000, -0.01 + 0.15 * i, \"component %i\" % (i + 1), color=c)\n",
    "\n",
    "    plt.ylim(-0.2, 0.6)\n",
    "    \n",
    "plt.xlabel('wavelength (Angstroms)')\n",
    "plt.ylabel('scaled flux + offset')\n",
    "plt.title('Mean Spectrum and Eigen-spectra')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXHBjBgxqGe9"
   },
   "source": [
    "We can estimate the contribution of each component by using the property \"explained variance ratio\".\n",
    "\n",
    "These are simply the eigenvalues of the covariance matrix, and give an idea of how much each component contributes to the total variance in the data. Their cumulative sum (plotted a few cells below) gives the progressively increasing explained variance ratio for a given number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sf4fHIGPqGe9",
    "outputId": "5538fa95-82cd-4760-945d-e575705274b6"
   },
   "outputs": [],
   "source": [
    "pca_50.explained_variance_ratio_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MZ_s6y8qGe9"
   },
   "source": [
    "We can interpret the eigenvectors as the \"basis\" that explains most of the variability in the data. \n",
    "\n",
    "How can we know if this works? Let's reverse-engineer the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SoD7EVOqGe9"
   },
   "outputs": [],
   "source": [
    "Xrec_50 = pca_50.inverse_transform(X_proj_50) \n",
    "\n",
    "Xrec_100 = pca_100.inverse_transform(X_proj_100)\n",
    "\n",
    "Xrec_1000 = pca_1000.inverse_transform(X_proj_1000) #This is useful for a sanity check; there should be no information loss, modulo numerical precision issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the original spectra to the results of inverting the PCA. Comment out/Uncomment lines below to look at each version of PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vu86BPXVqGe-",
    "outputId": "f52ba8b6-4f24-4c9c-905b-00ea2e250915"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(4,8):\n",
    "    plt.subplot(2,2,i-3)\n",
    "    plt.plot(wavelengths, Xn[i], label = 'orig', c = 'k')\n",
    "    plt.plot(wavelengths, Xrec_50[i], '--', label = 'new, 50 PCs', c = 'g')\n",
    "    #plt.plot(wavelengths, Xrec_100[i], '--', label = 'new, 100 PCs', c = 'b')\n",
    "    #plt.plot(wavelengths, Xrec_1000[i], '--', label = 'new, 1000 PCs', c = 'r')\n",
    "    #plt.ylim(-0.5,0.5)\n",
    "    plt.legend();\n",
    "plt.xlabel('wavelength (Angstroms)');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's difficult to see the differences from these plots, so let's look at residuals instead. Comment out/Uncomment lines below to look at each version of PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(4,8):\n",
    "    plt.subplot(2,2,i-3)\n",
    "    plt.plot(wavelengths, (Xrec_50[i]-Xn[i])/Xn[i], '--', label = '% diff 50', c = 'g')\n",
    "    #plt.plot(wavelengths, (Xrec_100[i]-Xn[i])/Xn[i], '--', label = 'diff 100', c = 'b')\n",
    "    #plt.plot(wavelengths, (Xrec_1000[i]-Xn[i])/Xn[i], '-.', label = 'diff 1000', c = 'k')\n",
    "    plt.ylim(-0.5,0.5)\n",
    "    plt.legend();\n",
    "plt.xlabel('wavelength (Angstroms)');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZPAc2zMvcVT"
   },
   "source": [
    "Note that while many spectra are well represented by (for example) 50 components, some are not! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQzsfgNFqGe-"
   },
   "source": [
    "### How can we know what's a good number of components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7CFvmppqGe-"
   },
   "source": [
    "To get an idea, we can plot the \"explained_variance_ratio\" property of the PCA decomposition. It looks a lot like the Elbow Method, but upside down; in particular, the variance explained by N components always increases with N, but there is usually a spot after which the returns tend to diminish. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VISfPMzZqGe-",
    "outputId": "1868eb77-cdd4-4faf-ef28-7e384d8c671b"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca_1000.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');\n",
    "plt.xlim(0,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6ja5Fa5qGe-"
   },
   "source": [
    "### Question:\n",
    "What would you recommend in the case above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRnmL50wqGe-"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UawZDW3xqGe-"
   },
   "source": [
    "#### IMO, for science problems, how many components you need (and whether PCA is the right way to go about it) really depends on the scope. Unsupervised dimensionality reduction methods, such as PCA, are insensitive to the purpose, which is not always great. \n",
    "    \n",
    "#### For example, in the case of the spectra above, while it is true that even a handful components capture a large fraction of the variance, the percent difference between the original spectra and the reconstructed ones can be large in the vicinity of \"spiky\" features, and also vary greatly from object to object. Whether this is acceptable for your particular science goal, only you can tell! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StHQZUo1qGfA"
   },
   "source": [
    "### Let's now take a look at images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EP-f5Bp3qGfA"
   },
   "source": [
    "This data set is composed by 200 images randomy selected from the Kaggle Galaxy Zoo challenge:\n",
    "\n",
    "https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge\n",
    "\n",
    "The code below visualizes the first 25 objects in your data set. You can run it to get a view of the first 25 galaxies. Note: you might get an error message, in this case see here\n",
    "\n",
    "https://stackoverflow.com/questions/43288550/iopub-data-rate-exceeded-in-jupyter-notebook-when-viewing-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6aDXwzPqGfA"
   },
   "outputs": [],
   "source": [
    "#Takes < 1 minute, let's use non-bubbled images\n",
    "\n",
    "images = []\n",
    "for i in range(200):\n",
    "    img =skimage.io.imread('../Data/Images/Image_'+str(i)+'.png')\n",
    "    img_resized = resize(img,(100,100))\n",
    "    length = np.prod(img_resized.shape)\n",
    "    img_resized = np.reshape(img_resized,length)\n",
    "    images.append(img_resized)\n",
    "    \n",
    "images = np.vstack(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKWiajDyqGfA",
    "outputId": "2b58f248-0880-4d7c-afec-dfdc6863fb03"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols= 5, nrows = 5,figsize=(50,50))\n",
    "\n",
    "ax = axes.ravel()\n",
    "\n",
    "for i in range(ax.shape[0]):\n",
    "\n",
    "    img = skimage.io.imread('../Data/Images/Image_'+str(i)+'.png')\n",
    "    ax[i].imshow(img, cmap='gray')\n",
    "    ax[i].set_xticks([])\n",
    "    ax[i].set_yticks([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6ip1bNeqGfA"
   },
   "source": [
    "### Here, we do the PCA decomposition on each of the RGB channels separately. I'm not sure of whether it is optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2-0bSBJqGfB"
   },
   "outputs": [],
   "source": [
    "r_images = images.reshape(200, -1,  3)[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfnjwF8lqGfB",
    "outputId": "2e5eb7b7-9fb3-44c6-e8c6-2c82eef5a103"
   },
   "outputs": [],
   "source": [
    "r_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nv0YrbqHqGfB"
   },
   "outputs": [],
   "source": [
    "#Calling PCA on images\n",
    "\n",
    "estimator = decomposition.PCA(n_components=100)\n",
    "\n",
    "r_images_PCA = estimator.fit_transform(r_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FQrUDj9qGfB",
    "outputId": "1ef4b504-12a1-4f4d-b70c-851d67a15cf5"
   },
   "outputs": [],
   "source": [
    "#This tell us about the dimensionality reduction we have achieved.\n",
    "\n",
    "r_images_PCA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVGaUlO7qGfB"
   },
   "outputs": [],
   "source": [
    "components = estimator.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eP-d9dH8qGfC"
   },
   "source": [
    "### We can plot the first 50 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkA6yd1_qGfC",
    "outputId": "4cca3e2c-eb59-4615-d014-7c7894964543"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 10, figsize=(12, 6),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow((estimator.components_[i].reshape(100, 100)), cmap='bone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "- From this plot, what would you guess as an optimal number of components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jq5I_fwVqGfC"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeDjjLI0qGfC"
   },
   "source": [
    "We can use the explained variance ratio to see if there is an obvious optimal number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bngONhaJqGfC",
    "outputId": "50abb4cf-6d87-4084-9e3d-38cfea9506e0"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(estimator.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLPsTaoXqGfC"
   },
   "source": [
    "### Question:\n",
    "Note how different this plot is, compared to the case above, with the variance more equally distributed among many components. Approximately how many components would you need to retain ~ 90% of the variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qspjbQrVqGfD"
   },
   "source": [
    "### Let's now reconstruct the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F77zjhnLqGfD"
   },
   "outputs": [],
   "source": [
    "r_projected = estimator.inverse_transform(r_images_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHw8C-OBqGfD",
    "outputId": "eb6f5eef-668b-4994-cca2-278f7ecc43b0"
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(2, 10, figsize=(15, 5),\n",
    "                       subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                       gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i in range(10):\n",
    "    ax[0, i].imshow(r_images[i].reshape(100, 100), cmap='gray')\n",
    "    ax[1, i].imshow(r_projected[i].reshape(100, 100), cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrfUBtn6qGfD"
   },
   "source": [
    "We can do it for the three channels at once and then join them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_C6ViDKzqGfD"
   },
   "outputs": [],
   "source": [
    "estimator = decomposition.PCA(n_components=100) \n",
    "\n",
    "r_images = images.reshape(200, -1,  3)[:,:,1]                     \n",
    "estimator.fit(r_images)\n",
    "r_images_PCA = estimator.fit_transform(r_images)\n",
    "r_projected = estimator.inverse_transform(r_images_PCA)\n",
    "\n",
    "g_images = images.reshape(200, -1,  3)[:,:,1]                     \n",
    "estimator.fit(g_images)\n",
    "g_images_PCA = estimator.fit_transform(g_images)\n",
    "g_projected = estimator.inverse_transform(g_images_PCA)\n",
    "\n",
    "b_images = images.reshape(200, -1,  3)[:,:,2]                     \n",
    "estimator.fit(b_images)\n",
    "b_images_PCA = estimator.fit_transform(b_images)\n",
    "b_projected = estimator.inverse_transform(b_images_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VdofBM4zqGfD",
    "outputId": "4bb12c91-8179-4d5a-ec43-11afb54f5655"
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(2, 5, figsize=(50, 20),\n",
    "                       subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                       gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i in range(5):\n",
    "    ax[0, i].imshow((np.dstack([r_images[i].reshape(100, 100)*255, g_images[i].reshape(100, 100)*255, \n",
    "        b_images[i].reshape(100,100)*255]).astype(np.uint8)))\n",
    "    ax[1, i].imshow((np.dstack([r_projected[i].reshape(100, 100)*255, g_projected[i].reshape(100, 100)*255, \n",
    "        b_projected[i].reshape(100,100)*255]).astype(np.uint8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "What do you observe in these images? What part of the images is the PCA \"prioritizing,\" from what you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESoFxFBxqGfD"
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1zPadYZqGfE"
   },
   "source": [
    "Dimensionality reduction techniques are useful both to build understanding of what's in the data and to make sizes more manageable.\n",
    "\n",
    "Clustering and dimensionality reduction have a lot of overlap! See e.g. the Example # 2 of:\n",
    "\n",
    "https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.11-K-Means.ipynb\n",
    "\n",
    "You'll also see the combination in action on HW 4. \n",
    "\n",
    "#### Non-linear manifold techniques (t-SNE, Self Organized Maps...) are popular tools for visualization in 2D space, and they are useful for data exploration/investigation. However, they have tunable parameters that are not easy to tune, and they are hard to interpret.\n",
    "\n",
    "See for example:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/manifold.html#manifold\n",
    "\n",
    "or\n",
    "\n",
    "https://www.superdatascience.com/blogs/the-ultimate-guide-to-self-organizing-maps-soms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9755K67qGfE"
   },
   "source": [
    "### Acknowledgement Statement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for today, go ahead and upload this studio to Gradescope!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PHYS448",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
