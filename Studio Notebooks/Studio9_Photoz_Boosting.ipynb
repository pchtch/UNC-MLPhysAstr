{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U73xI_PhhOKG"
   },
   "source": [
    "### Boosting\n",
    "\n",
    "In this notebook, we'll test out boosting on the same Photometric Redshift data set that we used in the previous studio. \n",
    "\n",
    "We'll start with an adaptive boosting method (AdaBoost) and a Gradient Boosting method (GBM). Then we'll see how other gradient boosting methods, like HistGBM, compare in terms of performance and training speed. Along the way, we'll learn to use a Random Search for hyperparameter optimization. \n",
    "\n",
    "Copyright: Viviana Acquaviva (2023); see also other data credits below.\n",
    "\n",
    "Modifications and additions by Julieta Gruszko (2025).\n",
    "\n",
    "License: [BSD-3-clause](https://opensource.org/license/bsd-3-clause/)\n",
    "\n",
    "We are using [this paper](https://arxiv.org/abs/1903.08174), for which the data are public and available [here](http://d-scholarship.pitt.edu/36064/), for inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uTYRt0sLhOKH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "font = {'size'   : 16}\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14) \n",
    "#matplotlib.rcParams.update({'figure.autolayout': True})\n",
    "matplotlib.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTXfzhllhOKI"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate, KFold, cross_val_predict, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn import stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWr49WMnhOKJ"
   },
   "source": [
    "Reference for comparison of weak learners as base estimators:\n",
    "\n",
    "https://link.springer.com/chapter/10.1007/978-3-642-20042-7_32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8-1NwIohOKJ"
   },
   "source": [
    "### Reading in Data \n",
    "\n",
    "We can read in the photometric redshifts data set with the selection criteria applied in our last studio. I went ahead and saved it to the \"Data\" folder for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFPuoRX4hOKJ"
   },
   "outputs": [],
   "source": [
    "sel_features = pd.read_csv('../Data/sel_features.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sT_ME-O2hOKK"
   },
   "outputs": [],
   "source": [
    "sel_target = pd.read_csv('../Data/sel_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gjfdehm5hOKK",
    "outputId": "791fa796-4460-49f0-ea63-95e8c952c5dd"
   },
   "outputs": [],
   "source": [
    "sel_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLoaSet3hOKL",
    "outputId": "5d51d304-9fa1-4070-feef-5601fff19514"
   },
   "outputs": [],
   "source": [
    "sel_target.values.ravel() #changes shape to 1d row-like array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlS9qVwEhOKL"
   },
   "source": [
    "### We can try our usual benchmarking with AdaBoost, using default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "reNMJb7KhOKL"
   },
   "outputs": [],
   "source": [
    "model = AdaBoostRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riKoe2NDhOKL"
   },
   "outputs": [],
   "source": [
    "ypred = cross_val_predict(model, sel_features,sel_target.values.ravel(), cv = KFold(n_splits=5, shuffle=True, random_state=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I30XsaeGhOKL",
    "outputId": "b7b2ef17-25ab-4fdd-b2b0-7994500751bd"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(sel_target,ypred, s =10)\n",
    "plt.ylim(0,3)\n",
    "plt.ylabel(\"Predicted value\")\n",
    "plt.xlim(0,3)\n",
    "plt.xlabel(\"Target value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9WzlzTThOKL"
   },
   "source": [
    "### Questions: \n",
    "- What's the base estimator being boosted here? What is its max depth?\n",
    "- How many estimators are being used for the boosting?\n",
    "- Does the boosting seem to be working? Compare this result scatter plot to your single tree and Random Forest results from Studio 8. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if the boosting is working, we can study the performance as we increase the number of estimators. If our model is \"boost-worthy\" the results will improve with more estimators. sk-learn's $\\texttt{staged\\_predict}$ is a useful tool for this: it returns an array of ensemble predictions after each iteration of boosting. To use it, we need to set up a test-train split and use the $\\texttt{fit}$ method, since we want to monitor how the test accuracy changes at each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RkcnQImqhOKO"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(sel_features,sel_target.values.ravel(), test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9P4LFJwkjDW"
   },
   "source": [
    "We begin with a very weak learner: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHPKci4IhOKO"
   },
   "outputs": [],
   "source": [
    "model= AdaBoostRegressor(DecisionTreeRegressor(max_depth=3),\n",
    "                  n_estimators=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3iTYoRoNhOKP",
    "outputId": "4c91a511-4a26-49a6-cda2-a1e760f5b2d1"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmHhgUn6hOKP"
   },
   "source": [
    "We can plot the $R^2$ score between true and predicted values as a function of the number of stages/iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4ZtmwLKhOKP",
    "outputId": "2bcc9246-fd80-4362-a7b8-90a5e7c7ea82"
   },
   "outputs": [],
   "source": [
    "n_estimators = 30\n",
    "\n",
    "plt.plot(range(n_estimators), [metrics.r2_score(y_test,list(model.staged_predict(X_test))[i]) for i in range(n_estimators)], label = 'r2 score')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.ylim(0,1.0)\n",
    "\n",
    "plt.title('Max depth = 3')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3whoKM4hOKP"
   },
   "source": [
    "### We can now try again with a stronger base learner (max_depth = 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWBqpPnIhOKP",
    "outputId": "fb84af2e-2bb6-4ceb-a044-f03f83071e61"
   },
   "outputs": [],
   "source": [
    "n_estimators = 30\n",
    "\n",
    "model= AdaBoostRegressor(DecisionTreeRegressor(max_depth=6),\n",
    "                  n_estimators=n_estimators)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(sel_features,sel_target.values.ravel(), test_size=.3, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "plt.plot(range(n_estimators), [metrics.r2_score(y_test,list(model.staged_predict(X_test))[i]) for i in range(n_estimators)], label = 'r2')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.title('Base estimator, max depth = 6')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3SlMOZbhOKP"
   },
   "source": [
    "### And do the same with an even stronger base learner (max_depth = 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMB9DrARhOKQ",
    "outputId": "ad4624a7-c471-487c-8871-0f8b4f647cce"
   },
   "outputs": [],
   "source": [
    "n_estimators = 30\n",
    "\n",
    "model= AdaBoostRegressor(DecisionTreeRegressor(max_depth=10),\n",
    "                  n_estimators=30)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(sel_features,sel_target.values.ravel(), test_size=.3, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "plt.plot(range(n_estimators), [metrics.r2_score(y_test,list(model.staged_predict(X_test))[i]) for i in range(n_estimators)], label = 'r2')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.title('Base estimator, max depth = 10')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxE0yyUDhOKQ"
   },
   "source": [
    "### Let's combine all of them in one figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qDqrDPXohOKQ",
    "outputId": "e11041bd-371a-4c3d-b67d-1154c31bf608"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "n_estimators = 30\n",
    "\n",
    "for i, md in enumerate([3,6,10]):\n",
    "    \n",
    "    model = AdaBoostRegressor(DecisionTreeRegressor(max_depth=md),\n",
    "                  n_estimators=n_estimators)\n",
    "\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    plt.subplot(1,3,i+1)\n",
    "\n",
    "    plt.plot(range(n_estimators), [metrics.r2_score(y_test,list(model.staged_predict(X_test))[i]) for i in range(n_estimators)], label = 'r2 score', c = 'steelblue')\n",
    "\n",
    "    plt.xlabel('Iteration')\n",
    "\n",
    "    plt.ylim(0,1.0)\n",
    "\n",
    "    plt.title('Max depth = '+str(md)+', AdaBoost')\n",
    "    \n",
    "    if i == 2:\n",
    "        plt.legend();\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "#plt.savefig('AdaB_performance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "What is the \"boost-worthy\" threshold for Adaptive Boosting in this problem? (i.e. what base learners does boosting work for, from what you observe above?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHLS1jG8hOKQ"
   },
   "source": [
    "### We sort-of have an answer from the third panel of the figure above, but we could also ask whether we should keep boosting (i.e. if adding more stages is beneficial.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxfFVOWthOKQ",
    "outputId": "276c7178-d682-49ae-f664-cdfc61d8b741"
   },
   "outputs": [],
   "source": [
    "#Shall we keep boosting? (max_depth = 10)\n",
    "\n",
    "n_estimators = 60\n",
    "\n",
    "model= AdaBoostRegressor(DecisionTreeRegressor(max_depth=10),\n",
    "                  n_estimators=n_estimators)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(sel_features,sel_target.values.ravel(), test_size=.3, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "plt.plot(range(n_estimators), [metrics.r2_score(y_test,list(model.staged_predict(X_test))[i]) for i in range(n_estimators)], label = 'r2')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.title('Base estimator, max depth = 10')\n",
    "\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAx2uKbnhOKQ"
   },
   "source": [
    "### Questions: \n",
    "- Does adaptive boosting work with any base estimator? Or can estimators be too weak to work?\n",
    "- Does adding more boosting stages always improve the results?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rKnWdU1hOKQ"
   },
   "source": [
    "### Would this be true also for Gradient Boosted Trees algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGbt5-5UhOKQ"
   },
   "source": [
    "There is only one way to find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOKxoiRahOKR"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZk3jSkNhOKR"
   },
   "source": [
    "The parameters depend on the particular implementation.\n",
    "\n",
    "In the sklearn formulation, the parameters of each tree are essentially the same we have for Random Forests; additionally we have the \"learning_rate\" parameter, which dictates how much each tree contribute to the final estimator, and the \"subsample\" parameters, which allows one to use a < 1.0 fraction of samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "us9NaD8yhOKR"
   },
   "source": [
    "Let's test different tree depths with Gradient Boosting and see if there's a \"boost-worthy\" threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWCIkejGhOKR",
    "outputId": "cb9b7b2d-4da5-4f65-c82c-02632a03971d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "n_estimators = 30\n",
    "\n",
    "for i, md in enumerate([3,6,10]):\n",
    "    \n",
    "    model = GradientBoostingRegressor(max_depth=md,\n",
    "                  n_estimators=n_estimators)\n",
    "\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    plt.subplot(1,3,i+1)\n",
    "\n",
    "    plt.plot(range(n_estimators), [metrics.r2_score(y_test,list(model.staged_predict(X_test))[i]) for i in range(n_estimators)], label = 'r2 score', c = 'steelblue')\n",
    "\n",
    "    plt.xlabel('Iteration')\n",
    "\n",
    "    plt.ylim(0,1.0)\n",
    "\n",
    "    plt.title('Max depth = '+str(md)+', GBR')\n",
    "    \n",
    "    if i == 2:\n",
    "        plt.legend();\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "#plt.savefig('GBR_performance.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFAgfF9WhOKS"
   },
   "source": [
    "### Questions:\n",
    "- Does gradient boosting work with any base estimator? Or can estimators be too weak to work?\n",
    "- Does adding more boosting stages always improve the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization \n",
    "Now that we know what range of tree depth hyperparameters is interesting in each case, let's run hyperparameter optimization, examine the best-performing models, and compare the results. We'll also compare to the RandomForests approach from last studio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This took 1m and 3 sec on my laptop\n",
    "parameters = {'estimator__max_depth':[6,10,None], 'loss':['linear','square'], 'n_estimators':[20,50,100], 'learning_rate': [0.3,0.5,1.0]}\n",
    "nmodels = np.prod([len(el) for el in parameters.values()])\n",
    "model = GridSearchCV(AdaBoostRegressor(DecisionTreeRegressor()), parameters, \\\n",
    "                     cv = KFold(n_splits=5, shuffle=True, random_state = 5), \\\n",
    "                     verbose = 2, n_jobs = 4, return_train_score=True)\n",
    "model.fit(sel_features,sel_target.values.ravel())\n",
    "\n",
    "print('Best params, best score:', \"{:.4f}\".format(model.best_score_), \\\n",
    "      model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the winning model scores and parameters. We also want to look at the standard deviation of test scores, since we want to know what differences are statistically signficant when we compare models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(model.cv_results_)\n",
    "scoresCV = scores[['params','mean_test_score','std_test_score','mean_train_score']].sort_values(by = 'mean_test_score', \\\n",
    "                                                    ascending = False)\n",
    "scoresCV.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the standard deviation is 0.03 - giving us a hint of what's significant - and that a few different models have similar scores. If you change the random seed in the cross validation, the scores will change by a similar amount, and the best model may change as well.\n",
    "\n",
    "Additionally, the resulting scores will not be exactly reproducible because there is another random component in the adaptive learning set (this means that if you run the cross_validate function using the best model from above, you might get a different average score!)\n",
    "\n",
    "We can also look at the worst-performing models, to see which hyperparameters are having a strong effect:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoresCV.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- Give a brief description of the hyperparameters being varied and the values being tested for each. \n",
    "- What hyperparameter(s) have a strong effect on the adaptive boosting performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's pick the best model and check the scores. We should do nested cross validation to get the generalization errors right - but if we are just comparing models, this is ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bm = cross_val_predict(bm, sel_features, sel_target.values.ravel(), cv = KFold(n_splits=5, shuffle = True, random_state=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate outlier fraction and NMAD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(len(np.where(np.abs(sel_target.values.ravel()-y_pred_bm)>0.15*(1+sel_target.values.ravel()))[0])/len(sel_target.values.ravel()),3))\n",
    "\n",
    "print(np.round(1.48*np.median(np.abs(sel_target.values.ravel()-y_pred_bm)/(1 + sel_target.values.ravel())),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are actually better than what we obtained for the Random Forests model! But is the difference statistically significant? One way to explore this is by generating several sets of predictions, and calculating their standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = np.random.choice(100,8, replace = False) #pick 8 different seeds\n",
    "\n",
    "olf = np.zeros(8)\n",
    "NMAD = np.zeros(8)\n",
    "\n",
    "for i in range(8): #A bit rough, but it gives a sense of what happens by varying the random seeds!\n",
    "    print('Iteration', i) #this is just to see the progress.\n",
    "    ypred = cross_val_predict(bm, sel_features, sel_target.values.ravel(), cv = KFold(n_splits=5, shuffle=True, random_state=seeds[i]))\n",
    "    olf[i] = len(np.where(np.abs(sel_target.values.ravel()-ypred)>0.15*(1+sel_target.values.ravel()))[0])/len(sel_target.values.ravel())\n",
    "    NMAD[i] = 1.48*np.median(np.abs(sel_target.values.ravel()-ypred)/(1 + sel_target.values.ravel()))\n",
    "\n",
    "print('OLF avg/std:, {0:.5f}, {1:0.5f}'.format(olf.mean(), olf.std()))\n",
    "print('NMAD avg/std:, {0:.5f}, {1:0.5f}'.format(NMAD.mean(), NMAD.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the same approach in Studio 8, we found that using Random Forests, our results were:\n",
    "\n",
    "OLF avg/std:, 0.05708, 0.00081\n",
    "\n",
    "NMAD avg/std:, 0.03687, 0.00041\n",
    "\n",
    "\n",
    "### Question: \n",
    "Which method would you recommend for this problem: Adaptive Boosting or Random Forest? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next step is to compare Adaptive Boosting with different Gradient Boosted Trees algorithms. \n",
    "\n",
    "We begin by using sklearn's GBM, then we move on to the lighter version, HistGBM, and finally we consider one of the most popular GBT-based algorithm, XGBoost.\n",
    "\n",
    "We also take a look at the possibility of using a Randomized Search instead of a Grid Search in order to speed up our optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can run the optimization process for this algorithm on a similar grid to the one used for AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# This took about 2.5 minutes on my machine\n",
    "\n",
    "parameters = {'max_depth':[6,10,None], 'loss':['squared_error','absolute_error'], \n",
    "              'n_estimators':[20,50,100], 'learning_rate': [0.1,0.3,0.5]}\n",
    "nmodels = np.prod([len(el) for el in parameters.values()])\n",
    "model = GridSearchCV(GradientBoostingRegressor(), parameters, \n",
    "                     cv = KFold(n_splits=5, shuffle=True, random_state = 5), \\\n",
    "                     verbose = 2, n_jobs = 4, return_train_score=True)\n",
    "model.fit(sel_features,sel_target.values.ravel())\n",
    "\n",
    "print('Best params, best score:', \"{:.4f}\".format(model.best_score_), \\\n",
    "      model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are comparable to what we obtained with AdaBoost (slightly lower, typically). We can check what happens to the outlier fraction and NMAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bm = cross_val_predict(bm, sel_features, sel_target.values.ravel(), cv = KFold(n_splits=5, shuffle = True, random_state=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(len(np.where(np.abs(sel_target.values.ravel()-y_pred_bm)>0.15*(1+sel_target.values.ravel()))[0])/len(sel_target.values.ravel()),3))\n",
    "\n",
    "print(np.round(1.48*np.median(np.abs(sel_target.values.ravel()-y_pred_bm)/(1 + sel_target.values.ravel())),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the performance of the two algorithms is similar, but one important difference is *timing*. To explore exactly the same parameter space, GBR took ~ 2 times longer than AdaBoost. Additionally, gradient boosted methods typically require more estimators, and we should explore more regularization parameters (e.g. subsampling) as well. In a nutshell, it would be great to speed up things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we make things faster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve on the time constraints in two ways: by switching to the histogram-based version of Gradient Boosting Regressor, and by using a Random Search instead of a Grid Search.\n",
    "\n",
    "HistGradientBoostingRegressor (inspired by [LightGBM](https://lightgbm.readthedocs.io/en/latest/)) works by binning the features into integer-valued bins (the default value is 256, but this parameter can be adjusted; note however that 256 is the maximum!), which greatly reduces the number of splitting points to consider, and results in a vast reduction of computation time, especially for large data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# This took ~ 53s on my laptop\n",
    "\n",
    "parameters = {'max_depth':[6,10,None], 'loss':['squared_error','absolute_error'], \n",
    "              'max_iter':[20,50,100], 'learning_rate': [0.1,0.3,0.5]}\n",
    "nmodels = np.prod([len(el) for el in parameters.values()])\n",
    "model = GridSearchCV(HistGradientBoostingRegressor(), parameters, \n",
    "                     cv = KFold(n_splits=5, shuffle=True, random_state = 5), \\\n",
    "                     verbose = 2, n_jobs = 4, return_train_score=True)\n",
    "model.fit(sel_features,sel_target.values.ravel())\n",
    "\n",
    "print('Best params, best score:', \"{:.4f}\".format(model.best_score_), \\\n",
    "      model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(model.cv_results_)\n",
    "scoresCV = scores[['params','mean_test_score','std_test_score','mean_train_score']].sort_values(by = 'mean_test_score', \\\n",
    "                                                    ascending = False)\n",
    "scoresCV.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for this relatively small data set, this is much faster (about 3x faster than GradientBoostingRegressor), giving us a chance to explore a wider parameter space (e.g. more trees, more options for learning rate). The trade-off is that we obtain a slight decrease in performance, compared with GBR. However, the standard deviation of test scores over the 5 CV folds suggests that this difference is not statistically significant. The speed-up from HistGBR is particularly large for large data sets (10,000 instances or more). \n",
    "\n",
    "Let's explore a larger set of models. Even with HistGBR, this is pretty slow, so I'll turn down the CV to 3-fold to speed things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "# This took 4 min 37 secs\n",
    "\n",
    "parameters = {'max_depth':[6,10,None], 'loss':['squared_error','absolute_error'], \n",
    "              'max_iter':[100,200,500], 'learning_rate': [0.05, 0.1,0.3,0.5], \n",
    "              'early_stopping':[True, False]}\n",
    "nmodels = np.prod([len(el) for el in parameters.values()])\n",
    "model = GridSearchCV(HistGradientBoostingRegressor(), parameters, cv = KFold(n_splits=3, shuffle=True), \\\n",
    "                     verbose = 2, n_jobs = 4, return_train_score=True)\n",
    "model.fit(sel_features,sel_target.values.ravel())\n",
    "\n",
    "print('Best params, best score:', \"{:.4f}\".format(model.best_score_), \\\n",
    "      model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to explore a wider parameter space here, but let's do one more thing to speed up the search: we can explore the hyperparameter space using Random Search instead of a Grid Search. \n",
    "\n",
    "Random Search is usually preferable when we have a high-dimensional parameter space; its use is not particularly warranted here.\n",
    "\n",
    "The number of iterations (the number of models that are considered) also needs to be adjusted, and depends on the dimensionality of the parameter space as well as the functional dependence of the loss function on the parameters. We will compare the timings with the cell above, where we explore 144 models, and only use 30 for the random search.\n",
    "\n",
    "\n",
    "The references here explores various ways of running a parameter search.\n",
    "\n",
    "Bergstra, J. and Bengio, Y., Random search for hyper-parameter optimization, The Journal of Machine Learning Research (2012)\n",
    "\n",
    "Bergstra, James, et al. \"Algorithms for hyper-parameter optimization.\" Advances in neural information processing systems 24 (2011).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "# 1 min 5 seconds\n",
    "\n",
    "parameters = {'max_depth':[6,10,None], 'loss':['squared_error','absolute_error'], \n",
    "              'max_iter':[100,200,500], 'learning_rate': [0.05, 0.1,0.3,0.5], \n",
    "             'early_stopping':[True, False]}\n",
    "nmodels = np.prod([len(el) for el in parameters.values()])\n",
    "model = RandomizedSearchCV(HistGradientBoostingRegressor(), parameters, cv = KFold(n_splits=3, shuffle=True), \\\n",
    "                     verbose = 2, n_jobs = 4, return_train_score=True, n_iter=30)\n",
    "model.fit(sel_features,sel_target.values.ravel())\n",
    "\n",
    "print('Best params, best score:', \"{:.4f}\".format(model.best_score_), \\\n",
    "      model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- What hyperparameters are we varying this time? Give a brief description (you may have to do some reading on sk-learn's documentation!).\n",
    "- What values are we testing, for each hyperparameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Randomized Search was able to find a comparably good solution in about 1/5 of the time. As we mentioned, the true gains of a Randomized Search pertain to exploring high-dimensional spaces. It is also possible to use the Randomized Search to find the general area of optimal parameters, and then refine the search in that neighborhood with a finer Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(model.cv_results_)\n",
    "scoresCV = scores[['params','mean_test_score','std_test_score','mean_train_score']].sort_values(by = 'mean_test_score', \\\n",
    "                                                    ascending = False)\n",
    "scoresCV.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the worst-performing models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoresCV.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "What hyperparameter(s) lead to better performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods also give you nice built-in ways to evaluate feature importance. You'll try that out on the homework!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgement statement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's it for Boosting, for now! Upload your completed notebook to Gradescope to submit it."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PHYS448",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
