{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIb7AW-J_Q0j"
   },
   "source": [
    "In this simple notebook we use a fully connected neural network to solve a previously seen problem in regression: the photometric redshift problem (see notebooks of Chapter 6 for more detail). We also explore some hyperparameter optimization strategies. \n",
    "\n",
    "It accompanies Chapter 8 of the book (2 of 2).\n",
    "\n",
    "Copyright: Viviana Acquaviva (2023).\n",
    "\n",
    "Modifications by Julieta Gruszko (2025)\n",
    "\n",
    "License: [BSD-3-clause](https://opensource.org/license/bsd-3-clause/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wCi2a2GB_Q0m"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ty1aqrju_Q0m"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "\n",
    "font = {'size'   : 16}\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14) \n",
    "matplotlib.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fAX7Ddfd_Q0n"
   },
   "outputs": [],
   "source": [
    "import torch as torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lzdIblJi_Q0n"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras import Input\n",
    "\n",
    "from keras.models import Sequential #the model is built adding layers one after the other\n",
    "\n",
    "from keras.layers import Dense #fully connected layers: every output talks to every input\n",
    "\n",
    "from keras.layers import Dropout #for regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLHnWtsJ_Q04"
   },
   "source": [
    "### Problem 2: photometric redshifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6-Ir38f_Q05"
   },
   "source": [
    "I will start out from the reduced (high-quality) data set we used for Bagging and Boosting methods. For reference, our best model achieved a NMAD around 0.02 and an outlier fraction of 4%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "e8hFO3qM_Q05"
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv('../Data/sel_features.csv', sep = '\\t')\n",
    "y = pd.read_csv('../Data/sel_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "100MCfNg_Q05"
   },
   "outputs": [],
   "source": [
    "X,y = shuffle(X,y, random_state = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a reminder of what's in here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7JEkymDS_Q05"
   },
   "outputs": [],
   "source": [
    "fifth = int(len(y)/5) #Divide data in fifths to use 60/20/20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "II0EN1pV_Q05"
   },
   "outputs": [],
   "source": [
    "X_train = X.values[:3*fifth,:]\n",
    "y_train = y[:3*fifth]\n",
    "\n",
    "X_val = X.values[3*fifth:4*fifth,:]\n",
    "y_val = y[3*fifth:4*fifth]\n",
    "\n",
    "X_test = X.values[4*fifth:,:]\n",
    "y_test = y[4*fifth:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aHqfJPv_Q05"
   },
   "source": [
    "We know that we need to scale our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1685403021297,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "lGcMKwtu_Q05",
    "outputId": "1bd10c4d-8302-408a-b51a-5e416d314aed"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "oaMj1-O0_Q06"
   },
   "outputs": [],
   "source": [
    "Xst_train = scaler.transform(X_train)\n",
    "Xst_val = scaler.transform(X_val)\n",
    "Xst_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1zYQ1yg_Q06"
   },
   "source": [
    "In a regression problem, we will choose a different activation for the output layer (e.g. linear), and an appropriate loss function (MSE, MAE, ...).\n",
    "\n",
    "Our input layer has six neurons for this problem.\n",
    "\n",
    "For other parameters and the network structure, we can start with two layers with 100 neurons and go from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t37u_TiV_Q06"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Tell subsequent layers what shape to expect\n",
    "model.add(keras.Input(shape=(6,)))\n",
    "\n",
    "optimizer = keras.optimizers.AdamW(learning_rate=0.001)\n",
    "\n",
    "# Add an input layer and specify its size (number of original features)\n",
    "\n",
    "model.add(Dense(100, activation='relu', input_shape=(6,)))\n",
    "\n",
    "# Add one hidden layer and specify its size\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "# Add an output layer \n",
    "\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mse', optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_pO8Jjl_Q06"
   },
   "source": [
    "We begin with 100 epochs and batch size = 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17829,
     "status": "ok",
     "timestamp": 1685403046028,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "EpC58myL_Q06",
    "outputId": "dba21fd1-fc42-4061-d3f0-33095bc5a504"
   },
   "outputs": [],
   "source": [
    "mynet = model.fit(Xst_train, y_train, validation_data= (Xst_val, y_val), epochs=100, batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1685403046029,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "R9F0vSIG_Q07",
    "outputId": "3acbecba-51eb-471b-ba8d-7b9f98ea56d5"
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(Xst_test, y_test)\n",
    "print('MSE:', results) #we are only monitoring the MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6o1--T2H_Q07"
   },
   "source": [
    "As usual, we can plot the loss throughout the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 982
    },
    "executionInfo": {
     "elapsed": 938,
     "status": "ok",
     "timestamp": 1685403046962,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "igJTaSwy_Q07",
    "outputId": "0375ff29-c1c5-4e19-850c-f88c516407cf"
   },
   "outputs": [],
   "source": [
    "plt.plot(mynet.history['loss'], label = 'train')\n",
    "plt.plot(mynet.history['val_loss'],'-.m', label = 'validation')\n",
    "plt.ylabel('Loss', fontsize = 14)\n",
    "plt.xlabel('Epoch', fontsize = 14)\n",
    "plt.legend(loc='upper right', fontsize = 12)\n",
    "plt.legend(fontsize = 12);\n",
    "#plt.savefig('Photoz_NN.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Npn26F0Uo0X3"
   },
   "source": [
    "### As always with regression problems, it is helpful to plot the predictions against the true values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3269,
     "status": "ok",
     "timestamp": 1685403054362,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "4j5LEBae_Q07",
    "outputId": "19fe2018-555e-4779-96a8-4ede8522a99f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "    \n",
    "plt.xlabel('True redshift', fontsize = 14)\n",
    "plt.ylabel('Estimated redshift', fontsize = 14)\n",
    "\n",
    "plt.scatter(y_test, model.predict(Xst_test), s =10, c = 'teal');\n",
    "\n",
    "plt.xlim(0,2)\n",
    "plt.ylim(0,2)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('Photoz_NN_scatter.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AISbbpfKo-nn"
   },
   "source": [
    "We didn't do cross validation, so we can only generate prediction on our single test fold in order to derive the other metrics we are interested in (OLF and NMAD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1685403054567,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "jB8PgXsG_Q08",
    "outputId": "d0001149-d329-4b15-d9de-8fdda7ad1926"
   },
   "outputs": [],
   "source": [
    "ypred = model.predict(Xst_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMxHlNxDTD-h"
   },
   "source": [
    "Calculate Outlier Fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1685403054567,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "Dz_2Oq5c_Q08",
    "outputId": "84eafa7c-7c08-4d6c-f0a3-a88bcd85cb6a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(np.where(np.abs(y_test-ypred)>0.15*(1+y_test))[0])/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RYDUSKL_Q08"
   },
   "source": [
    "Calculate Normalized Median Absolute Deviation (NMAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 208,
     "status": "ok",
     "timestamp": 1685403059612,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "wvPLrYvB_Q08",
    "outputId": "cecaeaa5-f74f-410a-b410-358758fe3897"
   },
   "outputs": [],
   "source": [
    "1.48*np.median(np.abs(y_test-ypred)/(1 + y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7rslnmK_Q08"
   },
   "source": [
    "We have decent, but not outstanding, numbers. We can play with/optimize the parameters; one thing that is very interesting IMO is to see the effect of using different losses on the residuals, and trying to add more layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fV8VkFC_Q08"
   },
   "source": [
    "### Let's try some optimization with keras tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "wvw7d9oK_Q09"
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from keras import layers\n",
    "\n",
    "#Some material below is adapted from the Keras Tuner documentation\n",
    "\n",
    "# https://keras-team.github.io/keras-tuner/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXOaqAvM_Q09"
   },
   "source": [
    "This function specifies which parameters we want to tune. Tunable parameters can be of type \"Choice\" (we specify a set), Int, Boolean, or Float.\n",
    "\n",
    "Keras-tuner has a lot of other options for how to set up hyperparameters, check out the API to see more: https://keras.io/keras_tuner/api/hyperparameters/\n",
    "It even has ways to set up conditional hyperparameters, so you don't explore meaningless combinations of hyperparameters (which was an issue with our sk-learn hyperparameter searches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "2XgKwB1J_Q09"
   },
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    for i in range(hp.Int('num_layers', 2, 6)): #We try between 2 and 6 layers\n",
    "        model.add(layers.Dense(units=hp.Int('units_' + str(i),\n",
    "                                            min_value=100, #Each of them has 100-300 neurons, in intervals of 100\n",
    "                                            max_value=300,\n",
    "                                            step=100),\n",
    "                               activation='relu'))\n",
    "    model.add(Dense(1, activation='linear')) #last one\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.AdamW(\n",
    "            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])), #And a few learning rates\n",
    "        loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "List the hyperparameters being optimized and briefly describe what each one does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmZr4Dgh_Q09"
   },
   "source": [
    "Next, we specify how we want to explore the parameter space. The Random Search is the simplest choice, but often quite effective; alternatives are Hyperband (optimized Random Search where a larger fraction of models is trained for a smaller number of epochs, but only the most promising ones survive), or Bayesian Optimization, which attempts to build a probabilistic interpretation of the model scores (the posterior probability of obtaining score x, given the values of hyperparameters).\n",
    "\n",
    "\n",
    "Normally, you'd try something like 40 models, at least, but this takes far too long to run for a studio (about 35 minutes, on my laptop). I've left the output of that full search below so you can see what happens, but we'll run an abbreviated search for now, so you can see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "XKTmAfeb_Q09"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "# This is an example of the settings you'd use for a more complete hyperparameter search\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=40, #number of combinations to try\n",
    "    executions_per_trial=3,\n",
    "    project_name='My Drive/Photoz') #may need to delete or reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "tuner_small = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=5, # too small for real use! Just showing you how it works\n",
    "    executions_per_trial=3,\n",
    "    project_name='My Drive/Photoz_small') #may need to delete or reset. This will store information about each trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- What is the \"objective\" parameter controlling? What is it being set to?\n",
    "- What is the \"executions per trial\" parameter controlling? Why is it set to something larger than 1? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfwvvlOE_Q09"
   },
   "source": [
    "We can visualize the search space below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1685403300374,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "GPJiiBOD_Q09",
    "outputId": "9daa13ac-4ce3-4b0e-f708-5ab3dde4b686"
   },
   "outputs": [],
   "source": [
    "tuner.search_space_summary() #for the full search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_small.search_space_summary() #for the abbreviated search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kU7aZ8CL_Q0-"
   },
   "source": [
    "Finally, it's time to put our tuner to work. \n",
    "\n",
    "\n",
    "This is a big job! \n",
    "\n",
    "### <span style=\"color:red\"> Do not run this cell, unless you want to be waiting for a long time.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "XrqAgLjg_Q0-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 40 Complete [00h 01m 12s]\n",
      "val_loss: 0.017915900175770123\n",
      "\n",
      "Best val_loss So Far: 0.015789744754632313\n",
      "Total elapsed time: 00h 49m 17s\n"
     ]
    }
   ],
   "source": [
    "tuner.search(Xst_train, y_train, #same signature as model.fit\n",
    "             epochs=100, validation_data=(Xst_val, y_val), batch_size=300, verbose = 1) \n",
    "\n",
    "#Note: setting verbosity to 0 would give no output until done - it took about ~50 mins on my laptop. \n",
    "#It was faster when I tried running with Adam instead of AdamW, interestingly enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> Instead, run this cell to see how it works.</span>\n",
    "\n",
    "It should take about 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_small.search(Xst_train, y_train, #same signature as model.fit\n",
    "             epochs=100, validation_data=(Xst_val, y_val), batch_size=300, verbose = 1) \n",
    "\n",
    "#Note: setting verbosity to 0 would give no output until done - it took about ~35 mins on my laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FlfCn9m_Q0-"
   },
   "source": [
    "The \"results\\_summary(n)\" function gives us access to the n best models. It's useful to look at a few because often the differences are minimal, and a smaller model might be preferable! Note that the \"number of units\" parameter would have a value assigned to it for each layer (even if the number of layers is smaller in that particular realization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">  Output for the full search, this cell won't execute if you haven't run it yourself! Don't try to run it.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1685403266983,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "9AeqNold_Q0-",
    "outputId": "e0995af6-9513-4580-dfb6-2339255ab4df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in ./My Drive/Photoz\n",
      "Showing 6 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 20 summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "units_0: 300\n",
      "units_1: 100\n",
      "learning_rate: 0.01\n",
      "units_2: 100\n",
      "units_3: 200\n",
      "units_4: 100\n",
      "units_5: 200\n",
      "Score: 0.015789744754632313\n",
      "\n",
      "Trial 22 summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 100\n",
      "units_1: 100\n",
      "learning_rate: 0.01\n",
      "units_2: 100\n",
      "units_3: 200\n",
      "units_4: 100\n",
      "units_5: 100\n",
      "Score: 0.016055627415577572\n",
      "\n",
      "Trial 12 summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "units_0: 300\n",
      "units_1: 100\n",
      "learning_rate: 0.01\n",
      "units_2: 100\n",
      "units_3: 200\n",
      "units_4: 200\n",
      "units_5: 200\n",
      "Score: 0.017035806862016518\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 300\n",
      "units_1: 300\n",
      "learning_rate: 0.01\n",
      "units_2: 300\n",
      "units_3: 200\n",
      "units_4: 300\n",
      "units_5: 100\n",
      "Score: 0.01709962698320548\n",
      "\n",
      "Trial 30 summary\n",
      "Hyperparameters:\n",
      "num_layers: 5\n",
      "units_0: 200\n",
      "units_1: 200\n",
      "learning_rate: 0.001\n",
      "units_2: 100\n",
      "units_3: 300\n",
      "units_4: 100\n",
      "units_5: 300\n",
      "Score: 0.017341187223792076\n",
      "\n",
      "Trial 29 summary\n",
      "Hyperparameters:\n",
      "num_layers: 5\n",
      "units_0: 300\n",
      "units_1: 100\n",
      "learning_rate: 0.001\n",
      "units_2: 200\n",
      "units_3: 100\n",
      "units_4: 100\n",
      "units_5: 300\n",
      "Score: 0.017379162833094597\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">  Output for your small search, go ahead and execute this one.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_small.results_summary(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "- Based on the results summary from the full tuning, is there much variation in the results of the best-performing model? Can we confidently say which the best-performing model is, with this information? Or is there something else we'd need to do?\n",
    "- Based on the results summary from the full tuning, approximately how many layers should our neural network have? Approximately how many neurons per layer? Feel free to give a range of values for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you can run these cells below, we'll use the best result from your small search, but later on I'll give you the hyperparameters for the best model from the full search to actually run the network and see its performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "dn06Jq-T_Q0-"
   },
   "outputs": [],
   "source": [
    "best_hps_small = tuner_small.get_best_hyperparameters()[0] #choose first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1685403313144,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "8qAU0mn1_Q0-",
    "outputId": "2e3e4027-0f5c-4bd0-8b06-2ae8aa90976b"
   },
   "outputs": [],
   "source": [
    "best_hps_small.get('learning_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1685403314663,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "5cLPW3MH_Q0-",
    "outputId": "879b0c4e-f09b-4660-c9a1-08378eea2d40"
   },
   "outputs": [],
   "source": [
    "best_hps_small.get('num_layers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1685403316549,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "9b_LsMsr_Q0-",
    "outputId": "c2121e09-824e-4f7a-d802-63311cc75c99"
   },
   "outputs": [],
   "source": [
    "#Size of layers\n",
    "\n",
    "print(best_hps_small.get('units_0'))\n",
    "print(best_hps_small.get('units_1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "bq8ULBCb_Q0_"
   },
   "outputs": [],
   "source": [
    "model = tuner_small.hypermodel.build(best_hps_small) #define model = best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "vilFKjD4_Q0_"
   },
   "outputs": [],
   "source": [
    "model.build(input_shape=(None,6)) #build best model (if not fit yet, this will give access to summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1685403346052,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "4nbegil5_Q0_",
    "outputId": "fa781e5c-65e6-45c8-e13e-f99c56c47eda"
   },
   "outputs": [],
   "source": [
    "model.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "Remember, you did a random search for hyperparameter optimization (and we ran a really small one), so answers to these questions will probably vary among your group!\n",
    "\n",
    "Using the small model search...\n",
    "- How many free parameters does your best model have? \n",
    "\n",
    "- Describe the structure of your best model: how many hidden layers, how many neurons per layer?\n",
    "\n",
    "- What is the learning rate for your best model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfhllAIK3kgn"
   },
   "source": [
    "Now, we'll switch to the best hyperparameters I found for you using the full search, and build a neural net with the optimal hyperparameters.\n",
    "\n",
    "There isn't a natural way to build the model using keras_tuner (as shown above for the small model), since we didnt use keras_tuner to find this model, so we'll just build it with keras, as we did at the beginning of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Tell subsequent layers what shape to expect\n",
    "model.add(keras.Input(shape=(6,)))\n",
    "\n",
    "optimizer = keras.optimizers.AdamW(learning_rate=0.01)\n",
    "\n",
    "# Add an input layer and specify its size (number of original features)\n",
    "\n",
    "model.add(Dense(300, activation='relu', input_shape=(6,)))\n",
    "\n",
    "# Add one hidden layer and specify its size\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "# Add another hidden layer and specify its size\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "# Add another hidden layer and specify its size\n",
    "\n",
    "model.add(Dense(200, activation='relu'))\n",
    "\n",
    "# Add an output layer \n",
    "\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mse', optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with 100 epochs and batch size = 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13992,
     "status": "ok",
     "timestamp": 1685403383570,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "E_gLSfuK_Q0_",
    "outputId": "2940c172-b79d-4ead-fb55-e60b59e1f882"
   },
   "outputs": [],
   "source": [
    "bestnet = model.fit(Xst_train, y_train, validation_data= (Xst_val, y_val), epochs=100, batch_size=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3P5FN4i44ANi"
   },
   "source": [
    "We can also look at the train vs validation curves for the optimal model found by the tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 999
    },
    "executionInfo": {
     "elapsed": 1183,
     "status": "ok",
     "timestamp": 1685403459512,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "ezQT_8iw_Q0_",
    "outputId": "dee604cf-60fb-4b05-d295-1077bc66159a"
   },
   "outputs": [],
   "source": [
    "plt.plot(bestnet.history['loss'], label = 'train')\n",
    "plt.plot(bestnet.history['val_loss'],'-.m', label = 'validation')\n",
    "plt.ylabel('Loss', fontsize = 14)\n",
    "plt.xlabel('Epoch', fontsize = 14)\n",
    "plt.ylim(0,0.1)\n",
    "plt.legend(loc='upper right', fontsize = 12)\n",
    "plt.legend(fontsize = 12);\n",
    "#plt.savefig('OptimalNN_Photoz.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_nGIXfd4HlF"
   },
   "source": [
    "Finally, we report test scores for all the metrics of interest (MSE, OLF, NMAD):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 881,
     "status": "ok",
     "timestamp": 1685403397380,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "_C9_ECTO_Q0_",
    "outputId": "8df68ebb-3d16-4452-f12f-11b3024e1cd7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(Xst_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 694,
     "status": "ok",
     "timestamp": 1685403418031,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "UbvdcdQs_Q1A",
    "outputId": "1589a3bc-b201-44ac-b795-8061f8777612"
   },
   "outputs": [],
   "source": [
    "ypred = model.predict(Xst_test)\n",
    "\n",
    "#Calculate OLF\n",
    "\n",
    "print('OLF', len(np.where(np.abs(y_test-ypred)>0.15*(1+y_test))[0])/len(y_test))\n",
    "\n",
    "#Calculate Normalized Median Absolute Deviation (NMAD)\n",
    "\n",
    "print('NMAD', 1.48*np.median(np.abs(y_test-ypred)/(1 + y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "- Are the results improved, relative to the initial baseline model? \n",
    "- Are the results improved, relative to your ensemble decision tree models?\n",
    "- Does this model have high variance?\n",
    "- What else would we want to do to get definite answers to these questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3avc6jx_Q1D"
   },
   "source": [
    "### Below, we show the effect of changing the loss function (MSE/MAE/MAPE), and we estimate the uncertainties in the estimates of OLF/NMAD, so we can decide whether the differences are significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DV1hJxvg_Q1D"
   },
   "source": [
    "#### The model is the best model I found above (it came from a Random Search, you might find a different one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 118603,
     "status": "ok",
     "timestamp": 1685404066736,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "mJytTFAS_Q1D",
    "outputId": "816b3a74-2957-4afe-b285-1ae4d46d3378"
   },
   "outputs": [],
   "source": [
    "#this took about 5 minutes to run on my laptop\n",
    "#Architecture stays the same\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Tell subsequent layers what shape to expect\n",
    "model.add(keras.Input(shape=(6,)))\n",
    "\n",
    "optimizer = keras.optimizers.AdamW(learning_rate=0.01)\n",
    "\n",
    "# Add an input layer and specify its size (number of original features)\n",
    "\n",
    "model.add(Dense(300, activation='relu', input_shape=(6,)))\n",
    "\n",
    "# Add one hidden layer and specify its size\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "# Add another hidden layer and specify its size\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "# Add another hidden layer and specify its size\n",
    "\n",
    "model.add(Dense(200, activation='relu'))\n",
    "\n",
    "# Add an output layer \n",
    "\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "#We use three different loss functions and repeat the training 4x\n",
    "\n",
    "for loss in ['mse','mae', 'mape']:\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.AdamW(learning_rate = 0.01),\n",
    "        loss=loss)\n",
    "\n",
    "    OLF = np.zeros(4)\n",
    "    NMAD = np.zeros(4)\n",
    "\n",
    "    for i in range(0,4): #let's do this 4 times and change only random weights initialization\n",
    "    \n",
    "        model.fit(Xst_train, y_train,\n",
    "             epochs=100,\n",
    "             validation_data=(Xst_val, y_val), batch_size=300, verbose = 0)\n",
    "\n",
    "        ypred = model.predict(Xst_test)\n",
    "\n",
    "        #Calculate OLF\n",
    "\n",
    "        OLF[i] = len(np.where(np.abs(y_test-ypred)>0.15*(1+y_test))[0])/len(y_test)\n",
    "\n",
    "        #Calculate Normalized Median Absolute Deviation (NMAD)\n",
    "        \n",
    "        NMAD[i] = 1.48*np.median(np.abs(y_test-ypred)/(1 + y_test))\n",
    "\n",
    "    print('OLF mean/std using loss', loss, 'is:', \"{:.3f}\".format(OLF.mean()), \"{:.3f}\".format(OLF.std()))\n",
    "    print('NMAD mean/std using loss', loss, 'is:', \"{:.2f}\".format(NMAD.mean()), \"{:.3f}\".format(NMAD.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "- What are the source(s) of the variance we've measured in this test? E.g. in the past, we've measured the variance associated with changing the test set by using CV. Is that what we're doing here? If not, what causes the difference between the models we're using to get the variance on each OLF/NMAD result?\n",
    "\n",
    "- If we want to minimize OLF and NMAD, what loss (of these default options) should we use to train our model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opmL1SUZ_Q1D"
   },
   "source": [
    "### Acknowledgement Statement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're done, go ahead and upload both studio notebooks to Gradescope!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PHYS448_DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
