{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIb7AW-J_Q0j"
   },
   "source": [
    "In this simple notebook we use a fully connected neural network to solve a previously seen problem, the particle ID classification (see notebooks of Chapter 4 for more detail).\n",
    "\n",
    "It accompanies Chapter 8 of the book (1 of 2).\n",
    "\n",
    "Copyright: Viviana Acquaviva (2023).\n",
    "\n",
    "Modifications by Julieta Gruszko (2025).\n",
    "\n",
    "License: [BSD-3-clause](https://opensource.org/license/bsd-3-clause/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wCi2a2GB_Q0m"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ty1aqrju_Q0m"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "\n",
    "font = {'size'   : 16}\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14) \n",
    "matplotlib.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RELqQBID_Q0n"
   },
   "source": [
    "Pytorch is the most commonly used library for development of Deep Learning models. It is an open-source platform for python that also enables the use of multiple GPU platforms, like CUDA.\n",
    "\n",
    "Keras is a high-level API (Application Programming Interface) that is built on top of a deep learning library. Originally it was developed for tensorflow, but these days it supports multiple back-ends including pytorch. We can think of it as the equivalent of the sklearn library for neural networks. It is less general, and less customizable, but it is very user-friendly. \n",
    "\n",
    "Another option for similar API tools, built into pytorch directly, would be to use pytorch.nn . This is the more common option, but we'd lose access to some of the nice hyperparameter tuning options that interface with keras (and have to replace them with equivalent pytorch-compatible libraries, which do exist if you want to go that route). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAX7Ddfd_Q0n"
   },
   "outputs": [],
   "source": [
    "import torch as torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1685279974603,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "qN9_Zvz0_fE_",
    "outputId": "830bba80-ce66-4e32-a575-5d99c6632fcc"
   },
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how you set keras to use the pytorch back-end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzdIblJi_Q0n"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential #the model is built adding layers one after the other\n",
    "\n",
    "from keras.layers import Dense #fully connected layers: every output talks to every input\n",
    "\n",
    "from keras.layers import Dropout #for regularization\n",
    "\n",
    "from keras import Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AddTCBp6_Q0o"
   },
   "source": [
    "We begin with the 4top vs ttbar problem, and we use the configuration where we added the features \"number of leptons\", \"number of jets\" etc., but not the particle type for every track.\n",
    "\n",
    "For reference, the linear SVM with those features achieved 94.2% +/- 0.6% accuracy. Note that those numbers had not been run through <b> nested </b> cross validation so they might be slightly optimistic. \n",
    "\n",
    "Luckily we saved the data with our added feature engineering back then, we can just load it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdeqbK8B_Q0o"
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv('../Data/Features_lim_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-Qxlpua_Q0p"
   },
   "outputs": [],
   "source": [
    "y = np.genfromtxt('../Data/Labels_lim_2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1685280248203,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "QLOOvTME_Q0q",
    "outputId": "ffd54ac6-2d01-4e3d-b253-6ce538a96928"
   },
   "outputs": [],
   "source": [
    "X.values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to remind ourselves what's in there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eo9nE9rL_Q0r"
   },
   "source": [
    "There is no \"built-in\" cross validation process here, so we would need to build it ourselves. For now, we can build three sets: train, validation (for parameter optimization), and test (for final evaluation). We should ideally build this as a cross-validation structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGV2FTWj_Q0y"
   },
   "outputs": [],
   "source": [
    "#Always shuffle first\n",
    "\n",
    "X,y = shuffle(X,y, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XolI39tH_Q0y"
   },
   "outputs": [],
   "source": [
    "X_train = X.values[:3000,:]\n",
    "y_train = y[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLRFgtDm_Q0y"
   },
   "outputs": [],
   "source": [
    "X_val = X.values[3000:4000,:]\n",
    "y_val = y[3000:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nn046-as_Q0z"
   },
   "outputs": [],
   "source": [
    "X_test = X.values[4000:,:]\n",
    "y_test = y[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 130,
     "status": "ok",
     "timestamp": 1685280285412,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "ZZmgrG6Y_Q0z",
    "outputId": "1e8d8578-3476-4bb8-9b1c-e99a09d58179"
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6644VLi_Q0z"
   },
   "source": [
    "### Building the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think about the model architecture.\n",
    "\n",
    "For a fully-connected neural net, we always start with one neuron per feature at the input layer. At the output, we want to perform a classification task. One nice way to build a classifier is to output the probability of having membership in the positive class as the target value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "- How many neurons should our input layer have, in this case?\n",
    "- How many neurons should our output layer have, in this case?\n",
    "- What would be a reasonable differentiable activation function for our output layer? Hint: think about what range of values our network should return to work as a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ws9Ko4wv_Q0z"
   },
   "source": [
    "In addition to the input and output layers, we will add two hidden layers. Here we are making both sizes = 20 (we should optimize this hyperparameter!). We can also reserve the possibility of adding a dropout layer after each one, in case we decide we could benefit from regularization. The dropout fraction should also be optimized through CV.\n",
    "\n",
    "Other decisions that we have to make are: \n",
    "- which nonlinearities we use\n",
    "- which optimizer we use\n",
    "- the loss function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssa1M_xP_Q00"
   },
   "source": [
    "The commands below can be used to explore possible choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(keras.activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use ReLU for hidden layers, sigmoid for the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107,
     "status": "ok",
     "timestamp": 1685280325392,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "zUYax3cN_Q00",
    "outputId": "edf3b261-77f1-423d-8414-2f34e8487132"
   },
   "outputs": [],
   "source": [
    "dir(keras.optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdamW is a good \"default option\" to start with, we'll use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1685280328712,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "IlvBNdKG_Q00",
    "outputId": "844e4f40-8c55-4d95-9597-1614e82a4a1f"
   },
   "outputs": [],
   "source": [
    "dir(keras.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppQRvorFEKRc"
   },
   "source": [
    "A standard choice for a case like ours, where the labels are 0/1 but we can predict a probability, is the binary cross-entropy or log loss:\n",
    "\n",
    "L = - $\\frac{1}{N} \\sum_{i=1}^N y_i \\cdot log(p(y_i)) + (1-y_i) \\cdot log (1 - p(y_i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpxfJMetEn6K"
   },
   "source": [
    "p is the probability that an object belongs to the positive class. It penalizes positive examples that are associated with predicted low probability, and negative examples that are associated with predicted high probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to make initial choices for our training hyperparameters:\n",
    "- which starting learning rate we adopt: we'll start with 0.001, but again this should be decided through CV\n",
    "- the number of epochs: e.g. 100; we can plot quantities of interest to check that we have enough\n",
    "- the batch size for the gradient descent step: here 200, but can explore! Smaller batch sizes will be faster but less stable in terms of convergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "What is the difference between the number of epochs and the batch size? Describe how each is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otwhTPnm_Q01"
   },
   "source": [
    "### This is how we build a fully connected neural network in keras.\n",
    "\n",
    "The pytorch back-end throws a warning because of a bug here, but you can safely ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "woqxQqX1_Q01"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Tell subsequent layers what shape to expect. Keras can also infer this from the data, but it's slower\n",
    "model.add(keras.Input(shape=(24,)))\n",
    "\n",
    "# Add an input layer and specify its size (number of original features)\n",
    "\n",
    "model.add(Dense(20, activation='relu', input_shape=(24,)))\n",
    "\n",
    "# Add one hidden layer and specify its size\n",
    "\n",
    "model.add(Dense(20, activation='relu'))\n",
    "\n",
    "# Add an output layer \n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = keras.optimizers.AdamW(learning_rate=0.001)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfWlB-0NA6CM"
   },
   "source": [
    "The \"metric\" keyword here serves to specify other possible metrics we would like to monitor. The loss itself is not interpretable, so we'll keep an eye on the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "- How are the neuron weights being initialized for each layer?\n",
    "- How are the biases being initialized for each layer?\n",
    "\n",
    "Hint: You'll probably need to look at the Keras API documenation, found here: https://keras.io/api/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lkvPwRy_Q01"
   },
   "source": [
    "### Ready to fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22205,
     "status": "ok",
     "timestamp": 1685280461177,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "4a2GD11m_Q01",
    "outputId": "e74ce4e4-9722-4f5d-ad46-456ff7c6d4d6"
   },
   "outputs": [],
   "source": [
    "mynet = model.fit(X_train, y_train, validation_data= (X_val, y_val), epochs = 100,  batch_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gSnZtIbBXgQ"
   },
   "source": [
    "The train and validation accuracy don't look so good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmdX7xQI_Q01"
   },
   "source": [
    "It's helpful to plot how training and validation loss vary throughout the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "executionInfo": {
     "elapsed": 1445,
     "status": "ok",
     "timestamp": 1685280526737,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "ekaBUX5j_Q01",
    "outputId": "489da061-4d85-4185-f133-d46fb0b9457d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "\n",
    "plt.plot(mynet.history['loss'], label = 'train')\n",
    "plt.plot(mynet.history['val_loss'],'-.m', label = 'validation')\n",
    "plt.ylabel('Loss', fontsize = 14)\n",
    "plt.xlabel('Epoch', fontsize = 14)\n",
    "plt.legend(loc='upper right', fontsize = 12)\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "plt.plot(mynet.history['accuracy'], label = 'train')\n",
    "plt.plot(mynet.history['val_accuracy'], '-.m', label = 'validation')\n",
    "plt.ylabel('Accuracy', fontsize = 14)\n",
    "plt.xlabel('Epoch', fontsize = 14)\n",
    "plt.legend(fontsize = 12)\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "#plt.savefig('FirstNN.png', dpi= 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJLdMkyp_Q01"
   },
   "source": [
    "There's no sign of any convergence/improvement here! \n",
    "### Question:\n",
    "Any ideas to fix it? What step(s) did we skip?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JW2S9WCF_Q01"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXiEcuzRjcOP"
   },
   "source": [
    "When in doubt, take a look at the data again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "executionInfo": {
     "elapsed": 145,
     "status": "ok",
     "timestamp": 1685280664606,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "PzmnBG7__Q01",
    "outputId": "25f4500e-f3b9-4378-b275-d4e3a33802fe"
   },
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRv36Ye0tXMo"
   },
   "source": [
    "### Yep, we forgot scaling (and our features have wildly different ranges).\n",
    "\n",
    "We can still use our sk-learn tools to fix this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qQMC772_Q02"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T03xoxwA_Q02"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaUPRKycBmx0"
   },
   "source": [
    "As usual, we only use the training set to derive the scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1685280686426,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "qf-NT0U6_Q02",
    "outputId": "d3684d0a-9fdd-4a86-ba9d-47ab1b054ada"
   },
   "outputs": [],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nczuqWMB_Q03"
   },
   "outputs": [],
   "source": [
    "Xst_train = scaler.transform(X_train)\n",
    "Xst_val = scaler.transform(X_val)\n",
    "Xst_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lg0XCSX8kYaH"
   },
   "source": [
    "We can now train our neural network again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10651,
     "status": "ok",
     "timestamp": 1685280907189,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "ZIBbWjR9_Q03",
    "outputId": "cf4438d5-4d1a-4cb0-d551-815fbefdf88f"
   },
   "outputs": [],
   "source": [
    "mynet = model.fit(Xst_train, y_train, validation_data= (Xst_val, y_val), epochs=100, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2637,
     "status": "ok",
     "timestamp": 1685280917181,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "fjSveqjQ_Q03",
    "outputId": "68fff88e-7f67-4e0e-d66b-8dc4177adf4b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "\n",
    "plt.plot(mynet.history['loss'], label = 'train')\n",
    "plt.plot(mynet.history['val_loss'],'-.m', label = 'validation')\n",
    "plt.ylabel('Loss', fontsize = 14)\n",
    "plt.xlabel('Epoch', fontsize = 14)\n",
    "plt.legend(loc='upper right', fontsize = 12)\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "plt.plot(mynet.history['accuracy'], label = 'train')\n",
    "plt.plot(mynet.history['val_accuracy'], '-.m', label = 'validation')\n",
    "plt.ylabel('Accuracy', fontsize = 14)\n",
    "plt.xlabel('Epoch', fontsize = 14)\n",
    "plt.legend(fontsize = 12)\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "#plt.show()\n",
    "\n",
    "#plt.savefig('ScaledNN.png', dpi= 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- Do you observe high bias? What about high variance?\n",
    "- How does the validation accuracy compare to our SVM model with these features (reminder, that model had a test accuracy of 94.2% +/- 0.6%)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWx9wWQgkvIC"
   },
   "source": [
    "#### As you can see, this network is much better behaved, and it achieves a final accuracy similar to the one found by SVMs (this is common for tabular data like ours). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRhWSeUB_Q04"
   },
   "source": [
    "#### We do so see some signs of high variance in the accuracy/validation curve; some regularization technique, such as a Dropout layer, may help. Early stopping (not shown here) would also be worth trying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zF4Dll3N_Q04"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "# Tell subsequent layers what shape to expect\n",
    "model.add(keras.Input(shape=(24,)))\n",
    "# Add an input layer and specify its size (number of original features)\n",
    "\n",
    "model.add(Dense(20, activation='relu', input_shape=(24,)))\n",
    "\n",
    "model.add(Dropout(0.2)) #This is the dropout fraction\n",
    "\n",
    "# Add one hidden layer and specify its size\n",
    "\n",
    "model.add(Dense(20, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.2)) #This is the dropout fraction\n",
    "\n",
    "# Add an output layer \n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = keras.optimizers.AdamW(learning_rate=0.001)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11375,
     "status": "ok",
     "timestamp": 1685281165975,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "g5WAqdzI_Q04",
    "outputId": "ffcfc384-0927-43a5-87d1-0c5a4b271c6e"
   },
   "outputs": [],
   "source": [
    "mynet = model.fit(Xst_train, y_train, validation_data= (Xst_val, y_val), epochs=100, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "executionInfo": {
     "elapsed": 1820,
     "status": "ok",
     "timestamp": 1685281179753,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "__hX7-bv_Q04",
    "outputId": "c19d72a9-9c44-4b3c-d9dd-6422c6251163"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "\n",
    "plt.plot(mynet.history['loss'], label = 'train')\n",
    "plt.plot(mynet.history['val_loss'],'-.m', label = 'validation')\n",
    "plt.ylabel('Loss', fontsize = 14)\n",
    "plt.xlabel('Epoch', fontsize = 14)\n",
    "plt.legend(loc='upper right', fontsize = 12)\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "plt.plot(mynet.history['accuracy'], label = 'train')\n",
    "plt.plot(mynet.history['val_accuracy'], '-.m', label = 'validation')\n",
    "plt.ylabel('Accuracy', fontsize = 14)\n",
    "plt.xlabel('Epoch', fontsize = 14)\n",
    "plt.legend(fontsize = 12)\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "#plt.savefig('RegularizedNN.png', dpi= 300)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zX2-sCXSl1f5"
   },
   "source": [
    "### The final evaluation of the model is always done on the test set; the reason is that the validation fold is used for hyperparameter optimization (which we haven't done here), and test set is blind to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 332,
     "status": "ok",
     "timestamp": 1685281394378,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "pbWoRs0J_Q04",
    "outputId": "bbe2b144-24c5-4cc6-a5b2-6116456a1c92"
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(Xst_test, y_test, verbose=1)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 156,
     "status": "ok",
     "timestamp": 1685281414037,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "PtWa7nxw_Q04",
    "outputId": "a65b9234-8e03-4726-ab20-d39a1780267f"
   },
   "outputs": [],
   "source": [
    " #\"scores\" contains the test loss and the accuracy, which we are monitoring.\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- How does the performance of this 2 layer fully-connected neural network compare to the performance of the SVMs you tested?\n",
    "- Which one would you prefer to use: this fully-connected NN or the SVM? Give at least 2 arguments as for why your chosen method is the preferable one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgement Statement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you're done with this notebook, move on to the photometric redshift neural network example."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PHYS448_DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
