{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raN8Lv1fRETg"
   },
   "source": [
    "### Week 7 studio assignment: Linear Regression and Regularization\n",
    "This is a notebook exploring sk-learn's tools for linear regression. It covers:\n",
    "- using different metrics to evaluate regression performance\n",
    "- fitting with Ordinary Least Squares and Stochastic Gradient Descent\n",
    "- the effect of varying loss functions (L1, L2, and Huber)\n",
    "- how to introduce additional polynomial features\n",
    "- the effect of Lasso and Ridge regularization\n",
    "\n",
    "Copyright: Julieta Gruszko (2025) \n",
    "Some materials from Viviana Acquaviva (2023)\n",
    "\n",
    "License: [BSD-3-clause](https://opensource.org/license/bsd-3-clause/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3199,
     "status": "ok",
     "timestamp": 1684957560693,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "5vKUgjAqRETi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model #New!\n",
    "\n",
    "font = {'size'   : 16}\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14) \n",
    "#matplotlib.rcParams.update({'figure.autolayout': False})\n",
    "matplotlib.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YF6i47x2RETj"
   },
   "source": [
    "#### We begin by generating some data. We'll start with just 1 feature, and an underlying linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 177,
     "status": "ok",
     "timestamp": 1684957560869,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "6ojY0yPURETk"
   },
   "outputs": [],
   "source": [
    "np.random.seed(16) #set seed for reproducibility purposes\n",
    "\n",
    "x = np.arange(100) \n",
    "\n",
    "yp = 3*x + 3 + 2*(np.random.poisson(3*x+3,100)-(3*x+3)) #generate some data with scatter following Poisson distribution \n",
    "                                                    #with exp value = y from linear model, centered around 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1076,
     "status": "ok",
     "timestamp": 1684957561943,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "1FPIyshoRETl",
    "outputId": "f524e3d0-9536-4236-ed2c-611beeef7f08"
   },
   "outputs": [],
   "source": [
    "#Let's take a look!\n",
    "\n",
    "plt.scatter(x, yp);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmrUOBjJRETm"
   },
   "source": [
    "#### Here comes the linear regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1684957561944,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "DzIzW0lcRETm"
   },
   "outputs": [],
   "source": [
    "model = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "Take a quick look at the documentation for the model used above. \n",
    "- What fit method does it use?\n",
    "- What loss function does this model use? If it can use multiple different loss functions as options, list them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1684957561944,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "wb6KJVILRETn",
    "outputId": "c02bd73f-7dd9-4a3d-ab5d-3767cd9d6340"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0tIiaXRRETn"
   },
   "source": [
    "I can fit the model (right now, I will do it using the entire data set just to compare with the analytic solution). When only one predictor is present, I need to reshape it to column form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1684957561944,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "vMdwZgMbRETp",
    "outputId": "db8f2687-4ce9-4dc9-eb8a-9695191648d8"
   },
   "outputs": [],
   "source": [
    "model.fit(x.reshape(-1,1),yp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDOI7zVIRETq"
   },
   "source": [
    "The fitted model has attributes \"coef_\", \"intercept_\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1684957561944,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "E5B05UeYRETr"
   },
   "outputs": [],
   "source": [
    "slope, intercept = model.coef_, model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1684957561945,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "c4FS2yptRETr",
    "outputId": "44519bcd-9c9d-414d-942f-95bfa974cb09",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(slope, intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8c0tYdJRETr"
   },
   "source": [
    "We can plot the original and the fitted line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 924
    },
    "executionInfo": {
     "elapsed": 2152,
     "status": "ok",
     "timestamp": 1684957564090,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "_Kjfi3JIRETs",
    "outputId": "e69c0a1b-8e7c-4ff7-980e-13ab9901ad00"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "plt.scatter(x,yp, s = 20, c = 'gray', label = 'Data')\n",
    "plt.plot(x, slope*x + intercept, c ='k', label = 'Ordinary least squares fit')\n",
    "plt.plot(x, 3*x + 3, c = 'r', label = 'True regression line')\n",
    "plt.legend(fontsize = 14)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6Qr1CvFRETs"
   },
   "source": [
    "This matches the analytic prediction of the coefficients. Below, they're calculated from the data points with two methods: the expanded formula we derived in class to get the slope and intercept, and using numpy's variance and covariance functions to get the slope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1684957564090,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "CRdU77imRETs"
   },
   "outputs": [],
   "source": [
    "#the analytic formula\n",
    "\n",
    "theta1 = np.sum((x - np.mean(x))*(yp - np.mean(yp)))/np.sum((x - np.mean(x))*(x - np.mean(x)))\n",
    "\n",
    "theta0 = np.mean(yp) - theta1*np.mean(x)\n",
    "\n",
    "print('Theta_0, Theta_1:', theta0, theta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1684957564090,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "YGtKLU7cRETt",
    "outputId": "25133cd4-b451-4c73-cf19-6b79fa160621"
   },
   "outputs": [],
   "source": [
    "# using numpy's variance/covariance functions (note: a small difference is due to 1/n vs 1/(n-1) in the definition; use bias = True for consistency)\n",
    "print('Sample Cov / Sample var:', np.cov(x,yp, bias=True)[0,1]/np.var(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kc3mae7mRETu"
   },
   "source": [
    "#### We can (and should!) do cross validation and all the nice things we have learned to do for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1684957564091,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "LKdAuOhBRETu"
   },
   "outputs": [],
   "source": [
    "cv = KFold(n_splits = 5 , shuffle = True , random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1684957564091,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "r2Tk2fm4RETv"
   },
   "outputs": [],
   "source": [
    "scores = cross_validate(model, x.reshape(-1,1), yp, cv = cv, return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1684957564092,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "OYjW73FFRETv",
    "outputId": "3ffe9b22-206e-4966-ad21-03632fe60b47"
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1684957564092,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "6ahkW0ZvRETv",
    "outputId": "577effbc-ab9a-454b-a138-8ed5430dbf82"
   },
   "outputs": [],
   "source": [
    "print('Test scores:', '{:.3f}'.format(scores['test_score'].mean()), '{:.3f}'.format(scores['test_score'].std()))\n",
    "print('Train scores:', '{:.3f}'.format(scores['train_score'].mean()), '{:.3f}'.format(scores['train_score'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_wpoFvlRETw"
   },
   "source": [
    "### Questions: \n",
    "\n",
    "- What are the scores that are being printed out? \n",
    "\n",
    "Note: If you don't set a scoring method parameter, cross\\_validate returns whatever the default is for the model you passed it. You'll probably need to check documentation to find out what that is.\n",
    "\n",
    "- How are the scores? \n",
    "\n",
    "- Does it suffer from high variance? High bias?\n",
    "\n",
    "- What would happen to the scores if we increased the scatter (noise)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iXwCAblRETw"
   },
   "source": [
    "### <font color='green'> Scoring in regression problems. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGoqOSpXRETw"
   },
   "source": [
    "### Here is a way to visualize all the available scorers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1684957564092,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "F9iHQPWzRETw",
    "outputId": "2477be2d-d3bc-4254-f3f0-88b6f2a67743"
   },
   "outputs": [],
   "source": [
    "print(sorted(sklearn.metrics.get_scorer_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFlLjN_yRETx"
   },
   "source": [
    "### Do you recognize some of them?\n",
    "\n",
    "Let's try out the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1684957564092,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "pJ6V61TLRETx"
   },
   "outputs": [],
   "source": [
    "scores = cross_validate(model, x.reshape(-1,1), yp, cv = cv, scoring = 'neg_mean_squared_error', return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1684957603304,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "9Rx1pjRNRETx",
    "outputId": "ce5d59a9-f5c7-42dc-cf97-4d6cc78e8ca3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Test scores:','{:.3f}'.format(scores['test_score'].mean()), '{:.3f}'.format(scores['test_score'].std()))\n",
    "print('Train scores:','{:.3f}'.format(scores['train_score'].mean()), '{:.3f}'.format(scores['train_score'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_FkG-8iTj_-"
   },
   "source": [
    "Something to note is that estimators of performance of the \"error\" type (in other words, the lower, the better) receive a negative sign in sklearn. This is just to maintain consistency with the \"higher score = better\" framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMCuv3wvRETy"
   },
   "source": [
    "Can also try the Median Absolute Error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XAJQAinTRETy"
   },
   "outputs": [],
   "source": [
    "scores = scores = cross_validate(model, x.reshape(-1,1), yp, cv = cv, scoring = 'neg_mean_absolute_error', return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 177,
     "status": "ok",
     "timestamp": 1684957484902,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "-SwL3QUeRETz",
    "outputId": "267c7770-6699-40fe-ba90-4d32ea5fb8e5"
   },
   "outputs": [],
   "source": [
    "print('Test scores:','{:.3f}'.format(scores['test_score'].mean()), '{:.3f}'.format(scores['test_score'].std()))\n",
    "print('Train scores:','{:.3f}'.format(scores['train_score'].mean()), '{:.3f}'.format(scores['train_score'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oAbIyu2RETz"
   },
   "source": [
    "Finally, by plotting the residuals, we can see that they are not independent of x (the assumptions of the probabilistic linear model are not satisfied). But that doesn't mean we can't create a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 731,
     "status": "ok",
     "timestamp": 1684957514998,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "ToIBztrfRETz",
    "outputId": "88244d80-6857-4a53-fba2-9e7fe54b1be6"
   },
   "outputs": [],
   "source": [
    "plt.scatter(x, slope*x + intercept - yp, color = 'b', label = 'Residuals')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eahDqOwCRET1"
   },
   "source": [
    "#### Note: as we already discussed, so far we have not changed the loss function (MSE), or the coefficients of the model. We have only looked at different evaluation metrics.\n",
    "\n",
    "### Questions:\n",
    "- Would the best fit line change if we optimize a different loss function?\n",
    "- Will the fit method used above (the one used by sk-learn's $\\texttt{LinearRegression}$ model) work for a different loss function?\n",
    "- What are some options for implementing a fit using a different loss function? Give at least 2 methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already got the experience of writing a grid search in Week 1's studio, so we'll focus our attention on more efficient methods and use the built-in options from sk-learn. \n",
    "\n",
    "We'll be using stoachastic gradient descent, using sk-learn's SGDRegressor model, to try out different loss functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtmNjLmsRET5"
   },
   "source": [
    "However, because these data are so regular, it's kind of boring, so before trying the different losses let's inject some outliers in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvUMwykcRET5"
   },
   "source": [
    "### What happens when we add outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 144,
     "status": "ok",
     "timestamp": 1684957902476,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "a4aBR72IRET6"
   },
   "outputs": [],
   "source": [
    "np.random.seed(12) #set \n",
    "out = np.random.choice(100,15) #select 15 outliers indexes\n",
    "yp_wo = np.copy(yp)\n",
    "np.random.seed(12) #set again\n",
    "yp_wo[out] = yp_wo[out] + 5*np.random.rand(15)*yp[out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 836,
     "status": "ok",
     "timestamp": 1684957904412,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "BGezf4wARET6",
    "outputId": "88bf0234-61f4-427d-b92c-ce4ed1d7bbc5"
   },
   "outputs": [],
   "source": [
    "plt.scatter(x,yp_wo, label = 'Data + outliers')\n",
    "plt.scatter(x,yp, label = 'Original data')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hpbc_heARET6"
   },
   "source": [
    "We can see the effect for the MSE loss right away, still using the OLS method as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 153,
     "status": "ok",
     "timestamp": 1684957915703,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "C1OvnpR3RET7",
    "outputId": "8fef8aff-fc1f-42eb-c3af-3499cea71ce4"
   },
   "outputs": [],
   "source": [
    "model.fit(x.reshape(-1,1),yp_wo)\n",
    "\n",
    "slope, intercept = model.coef_, model.intercept_\n",
    "\n",
    "print(slope, intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "plt.scatter(x,yp_wo, s = 20, c = 'gray', label = 'Data')\n",
    "plt.plot(x, slope*x + intercept, c ='k', label = 'Ordinary least squares fit')\n",
    "plt.legend(fontsize = 14)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compare the result using SGD for MSE loss, to check for consistency. \n",
    "\n",
    "### Note:\n",
    "\n",
    "sk-learn's SGD struggles to find the best fit when the features or targets span a large range of values, since the default settings are designed for scaled data. Really, you should apply StandardScaler to $\\textbf{both}$ the features and targets first. \n",
    "This is different than what we've done in the past! Before, we only scaled the features, so we could rely on the pipeline object to do it for us. Now, we'd also want to scale the targets. \n",
    "\n",
    "The annoying thing about doing that is that it will force you to invert the scaling to get back coefficients that are meaningful in your original units. \n",
    "\n",
    "\n",
    "Another option is to tweak the SGD step size (initial learning rate), number of iterations, and possibly the learning rate to compensate for this. When you can get away with it, this is conceptually simpler, but it incurs a bigger computational cost. Changing other settings (like the loss function used) will change the initial values you need, so beware if you decide to do this! Using the verbose mode of SGDRegressor is useful for checking if the fit is actually working.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdmodel = linear_model.SGDRegressor(penalty=None, eta0=.0000001, max_iter=10000, tol=1e-5) \n",
    "#the default loss function used by SGD is MSE\n",
    "# We're turning off the regularization with penalty = 'None'\n",
    "# We're using a very small initial learning rate and extra iterations (defaults are .01 and 1000) since we didn't scale the data\n",
    "# We're using the default learning rate setting, which starts large and levels off as t^-0.25, where t is the iteration number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdmodel.fit(x.reshape(-1, 1), yp_wo.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdslope, sgdintercept =sgdmodel.coef_, sgdmodel.intercept_\n",
    "print(\"SGD MSE coefficients:\", sgdslope, sgdintercept)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not exactly the same, but pretty close! As a numerical method, SGD won't match the analytic solution exactly.\n",
    "\n",
    "Now that we have our SGD set up, we can try some different loss functions.\n",
    "\n",
    "### Varying loss functions\n",
    "\n",
    "We'll try out these loss functions:\n",
    "- L1 loss (MAE)\n",
    "- Huber loss (hybrid between L1 and L2)\n",
    "\n",
    "SGD doesn't explicitly have an L1 loss option, but Huber loss's epsilon parameter, which modifies where the transition occurs between L2 and L1 error, can be set to a very small value to mimic using an L1 loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "Why do you think it is that sk-learn's gradient descent method doesn't implement a purely-L1 loss? What could go wrong if it did?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, using L1-style loss:\n",
    "sgdmodel_l1 = linear_model.SGDRegressor(loss = \"huber\", epsilon = .001, eta0=.1, max_iter=10000, penalty=None, tol=1e-5) \n",
    "sgdmodel_l1.fit(x.reshape(-1, 1), yp_wo.flatten()) \n",
    "\n",
    "l1slope, l1intercept =sgdmodel_l1.coef_, sgdmodel_l1.intercept_\n",
    "print(\"Scaled L1 SGD coefficients:\", l1slope, l1intercept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now using Huber loss with epsilon of 500:\n",
    "sgdmodel_huber500 = linear_model.SGDRegressor(loss = \"huber\", eta0=.000001, max_iter=10000, penalty=None, epsilon=500) \n",
    "sgdmodel_huber500.fit(x.reshape(-1, 1), yp_wo.flatten()) \n",
    "\n",
    "huber500slope, huber500intercept =sgdmodel_huber500.coef_, sgdmodel_huber500.intercept_\n",
    "print(\"Huber SGD coefficients (epsilon = 100):\", huber500slope, huber500intercept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "plt.scatter(x,yp_wo, s = 20, c = 'gray', label = 'Data')\n",
    "plt.plot(x, sgdslope*x + sgdintercept, c ='b', label = 'SGD MSE fit')\n",
    "plt.plot(x, l1slope*x + l1intercept, c ='r', label = 'L1 fit')\n",
    "plt.plot(x, huber500slope*x + huber500intercept, c ='m', label = 'Huber fit, epsilon = 500')\n",
    "plt.legend(fontsize = 14)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOwpTmbSRET7"
   },
   "source": [
    "Note: the Huber loss is a hybrid between MSE and MAE (behaves like MAE when the error is larger than a certain amount, often called epsilon, so it's less sensitive to outliers). One possibility is to use the std of the y values to set epsilon.\n",
    "\n",
    "### Exercise and Question: \n",
    "Try a different value of epsilon in the Huber loss and check what happens to the resulting best fit. Make sure the fit is still working correctly with your new value (e.g. if you use verbose mode and see that there's < ~10 iterations, you may need to tweak settings).\n",
    "\n",
    "Explain what you see.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon is a hyperparameter that you can optimize like any other one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New features and the effect of regularization\n",
    "\n",
    "To study how regularization works, we need to have more features. \n",
    "\n",
    "We'll add a couple of new uncorrelated features, and some correlated features using the PolynomialFeatures method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_rjeC-WRET7"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = x #lets call our old feature x1\n",
    "#and add a new feature:\n",
    "x1 = np.logspace(2,3,num=100) \n",
    "\n",
    "#make the target values depend on both features\n",
    "ypb = 3*x0 + 2*x0**2 + 15*x1 + 3 + 5*(np.random.poisson(3*x0+2*x0**2 + 15*x1,100)-(3*x0 + 2*x0**2 + 15*x1)) \n",
    "                                                    #generate some data with scatter following Poisson distribution \n",
    "                                                    #with exp value = y from linear model, centered around 0\n",
    "\n",
    "xb = np.vstack((x0,x1)).T #stack the features into 2D array\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at plots of the target vs. each of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, sharey=True, figsize=(7, 3))\n",
    "ax[0].scatter(x0, ypb, marker='.')\n",
    "ax[0].set_xlabel('x0')\n",
    "ax[0].set_ylabel('target')\n",
    "ax[1].scatter(x1, ypb, marker='.')\n",
    "ax[1].set_xlabel('x1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add correlated features (polynomial transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(2, include_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_xb = poly.fit_transform(xb)\n",
    "new_xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly.get_feature_names_out()  # a convenient function to check the ordering of new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "- What features did we add using $\\texttt{PolynomialFeatures}$?\n",
    "- Which of those new features do you think may be useful in doing a LinearRegression fit on the data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sk-learn has many options for regression with regularization, including:\n",
    "- Using $\\texttt{SGDRegressor}$, set $\\texttt{penalty = \\{`l2', `l1', `elasticnet'\\}}$ : optimizes using stochastic gradient descent, with whichever regularizer you set (elasticnet is a combination of l1 and l2 with a parameter that governs the mix you use)\n",
    "\n",
    "- $\\texttt{Ridge}$ and $\\texttt{RidgeCV}$ are specifically for Ridge regularization, with many different optimizer options built-in. You can choose your optimizer manually, or use the default 'auto' option, which chooses an appropriate solver based on your data. The \"CV\" version has built-in cross-validation. \n",
    "\n",
    "- $\\texttt{Lasso}$ and $\\texttt{LassoCV}$: same as above, but for Lasso regularization\n",
    "\n",
    "### Let's start with Ridge regression, and compare the coefficients of the linear model for different amounts of regularization.\n",
    "\n",
    "\n",
    "This time we will apply scaling to the features to help the fits converge correctly, but we'll keep the coefficient re-scaling problem simpler by 1) not scaling y values and 2) not shifting the mean of each feature to 0 as part of scaling. That leaves us with just 1 re-scaling parameter we have to correct for. If you try this and have trouble with convergence, you can improve the scaling as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's pick alpha = 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's use Ridge to try something new, plus is should converge better then SGD\n",
    "ridgemodel = make_pipeline(StandardScaler(with_mean=False), Ridge(alpha=1000))\n",
    "\n",
    "ridgemodel.fit(new_xb,ypb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a pipeline model, you can access each part of the pipeline, in order, by index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To access the Scaler object:\n",
    "print(ridgemodel[0])\n",
    "#To access the Ridge model object:\n",
    "print(ridgemodel[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scaled coefficients: \", ridgemodel[1].coef_) # the scaled coefficients of the Ridge model\n",
    "\n",
    "coef_alpha_1000 = np.hstack([ridgemodel[1].coef_/ridgemodel[0].scale_, ridgemodel[1].intercept_])\n",
    "                         \n",
    "print(\"The coefficients and y intercept: \", coef_alpha_1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's see for alpha = 1.0.\n",
    "\n",
    "### Question: \n",
    "Make a prediction. Will the coefficients be larger or smaller?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: the notation changed in sklearn 1.2 and higher; \n",
    "#To reproduce book results, we need to use alpha = alpha * n_samples\n",
    "\n",
    "newmodel = make_pipeline(StandardScaler(with_mean=False), Ridge(alpha=1))\n",
    "\n",
    "newmodel.fit(new_xb,ypb)\n",
    "\n",
    "coef_alpha_1 = np.hstack([newmodel[1].coef_/newmodel[0].scale_, newmodel[1].intercept_])\n",
    "                         \n",
    "print(coef_alpha_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, below we use a trick to get coefficients for \"zero\" alpha (no regularization); we could have also used LinearRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newmodel = make_pipeline(StandardScaler(with_mean=False), Ridge(alpha=1e-11))\n",
    "\n",
    "newmodel.fit(new_xb,ypb)\n",
    "\n",
    "coef_alpha_noreg = np.hstack([newmodel[1].coef_/newmodel[0].scale_, newmodel[1].intercept_])\n",
    "                         \n",
    "print(coef_alpha_noreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison with the same procedure for the linear model (no regularization) shows consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodel = make_pipeline(StandardScaler(with_mean=False),linear_model.LinearRegression())\n",
    "lmodel.fit(new_xb,ypb)\n",
    "print(lmodel[1].coef_/lmodel[0].scale_, lmodel[1].intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's try Lasso Regularization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can compare the coefficients of the linear model for different amounts of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "plt.bar(np.arange(6)-0.2, np.abs(coef_alpha_1000), color = 'maroon',width=0.05, label = 'Ridge, alpha = 1000')\n",
    "plt.bar(np.arange(6)-0.1, np.abs(coef_alpha_1), color = 'orangered',width=0.05, label = 'Ridge, alpha = 1.0')\n",
    "plt.bar(range(6), np.abs(coef_alpha_noreg), color = 'grey',width=0.05, label = 'Linear (no regularization)')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xticks(np.arange(6), ['x0','x1', 'x0^2','x0x1','x1^2', 'Intercept'])  # Set text labels.\n",
    "\n",
    "plt.xlabel('Feature',fontsize=14)\n",
    "\n",
    "plt.ylabel('Coefficients (absolute value)',fontsize=14)\n",
    "\n",
    "plt.legend(fontsize=13);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's take a look at LASSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LassoCV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the coefficients for alpha = 10,000, alpha = 1,000 and alpha = 1. Lasso regularization tends to induce sparse coefficients, so we can check that that's true!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L10k = make_pipeline(StandardScaler(with_mean=False), Lasso(alpha = 10000, max_iter = 1000000, tol = 0.005))\n",
    "\n",
    "L10k.fit(new_xb, ypb)\n",
    "\n",
    "coef_L10k =  np.hstack([L10k[1].coef_, L10k[1].intercept_])\n",
    "\n",
    "print(coef_L10k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1000 = make_pipeline(StandardScaler(with_mean=False), Lasso(alpha = 1000, max_iter = 1000000, tol = 0.005))\n",
    "\n",
    "L1000.fit(new_xb, ypb)\n",
    "\n",
    "coef_L1000 =  np.hstack([L1000[1].coef_, L1000[1].intercept_])\n",
    "\n",
    "print(coef_L1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 = make_pipeline(StandardScaler(with_mean=False), Lasso(alpha = 1, max_iter = 1000000, tol = 0.005))\n",
    "\n",
    "L1.fit(new_xb, ypb)\n",
    "\n",
    "coef_L1 =  np.hstack([L1[1].coef_/L1[0].scale_, L1[1].intercept_])\n",
    "\n",
    "print(coef_L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we can plot all the coefficients together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "plt.bar(np.arange(6)-0.2, np.abs(coef_alpha_1000), color = 'maroon',width=0.05, label = 'Ridge, alpha = 1000')\n",
    "plt.bar(np.arange(6)-0.1, np.abs(coef_alpha_1), color = 'orangered',width=0.05, label = 'Ridge, alpha = 1.0')\n",
    "plt.bar(range(6), np.abs(coef_alpha_noreg), color = 'grey',width=0.05, label = 'Linear (no regularization)')\n",
    "plt.bar(np.arange(6)+0.1, np.abs(coef_L1), color = 'tab:cyan',width=0.05, label = 'Lasso, alpha = 1.0')\n",
    "plt.bar(np.arange(6)+0.2, np.abs(coef_L1000), color = 'tab:blue', width=0.05, label = 'Lasso, alpha = 1000')\n",
    "plt.bar(np.arange(6)+0.3, np.abs(coef_L10k), color = 'xkcd:indigo', width=0.05, label = 'Lasso, alpha = 10000')\n",
    "\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xticks(np.arange(6), ['x0','x1', 'x0^2','x0x1','x1^2', 'Intercept'])  # Set text labels.\n",
    "\n",
    "plt.xlabel('Feature',fontsize=14)\n",
    "\n",
    "plt.ylabel('Coefficients (absolute value)',fontsize=14)\n",
    "\n",
    "plt.legend(fontsize=13, bbox_to_anchor=(1.05, 1));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- In general, what was the effect of increasing alpha in Ridge regularization?\n",
    "- What was the effect of applying Lasso regularization with a high alpha (= 1000) parameter? What about applying it with very high alpha (= 10000)? In each case, were any coefficients set to 0? Which?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation for regularization:\n",
    "\n",
    "To actually pick an alpha, we should use cross-validation. sk-learn has built-in methods for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regm = make_pipeline(StandardScaler(with_mean=False),\n",
    "                     RidgeCV(alphas=np.logspace(-6,6,13), \\\n",
    "            cv = KFold(n_splits=5, shuffle=True, random_state=1),\\\n",
    "             scoring = 'neg_mean_squared_error'))\n",
    "\n",
    "regm.fit(new_xb,ypb); \n",
    "\n",
    "print('The best alpha is', regm[1].alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: LassoCV re-orders alphas in DESCENDING ORDER! Scores will be messed up unless you use model.alphas_ object\n",
    "\n",
    "lassomodel = make_pipeline(StandardScaler(with_mean = False), \\\n",
    "                      LassoCV(alphas = np.logspace(-1,4,6), \n",
    "                        cv = KFold(n_splits=5, shuffle=True, random_state=1), \\\n",
    "              max_iter = 10000000, tol = 1e-6))\n",
    "\n",
    "lassomodel.fit(new_xb,ypb)\n",
    "\n",
    "print('Best alpha:', lassomodel[1].alpha_)\n",
    "\n",
    "# Example of how to print all the results\n",
    "#print('Alphas', lassomodel[1].alphas_)\n",
    "#for i, alpha in enumerate(lassomodel[1].alphas_):\n",
    "#    print('Score for alpha', alpha, np.mean(lassomodel[1].mse_path_[i,:])) #for each alpha (row), mean of CV estimate of MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the Ridge and Lasso regularization are giving very different answers! This might be a case where we'd want to check how ElasticNet regularization behaves, since it allows us to vary smoothly between the two, optimizing the mix with a hyperparameter. You'll try that out on Homework 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgement Statement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once you're done, upload the completed studio to Gradescope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PHYS448",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
